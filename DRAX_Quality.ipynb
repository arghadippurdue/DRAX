{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "-xGTs6IwDtu1",
        "hVkJWuFyxef4",
        "2lEH6vby2zSW",
        "dZYIBlWo6L-8",
        "iUi-wSoD4bv-",
        "1uQUf9aw4id4",
        "n7joxKV5mdBX",
        "hkANxBRV42QX",
        "R0oJorClR41w",
        "WBJq9T73gBCD"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xGTs6IwDtu1"
      },
      "source": [
        "# Maxim Distiller torch 1.13"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qehm1jBSS8y2"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myHh-Pp7DtKm"
      },
      "outputs": [],
      "source": [
        "%cd /distiller-pytorch-1.13_mvcnn\n",
        "!pip3 install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHNufwVFapUA"
      },
      "outputs": [],
      "source": [
        "# Import fpzip for floating point compression\n",
        "!pip install fpzip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzfrO1ff0OZN"
      },
      "outputs": [],
      "source": [
        "# !pip install pandas --trusted-host pypi.org --trusted-host files.pythonhosted.org\n",
        "# !pip install --upgrade pip\n",
        "!pip install pyzfp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhroT3BVWf0l"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade --no-cache-dir gdown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVkJWuFyxef4"
      },
      "source": [
        "# Extract Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvTFL4akoZr4"
      },
      "outputs": [],
      "source": [
        "!tar xf modelnet_test.tgz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data"
      ],
      "metadata": {
        "id": "eunczM3U_2VX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lEH6vby2zSW"
      },
      "source": [
        "# Importing Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iednWLSK29CW"
      },
      "source": [
        "Importing libraries and parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsCo5XwsDi4f"
      },
      "outputs": [],
      "source": [
        "!pip --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n01gs6_M2sH_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy import array\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import os,shutil,json\n",
        "import argparse\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import pickle\n",
        "import os\n",
        "from torch.utils.tensorboard import SummaryWriter # Tensorboard\n",
        "import time\n",
        "\n",
        "import glob\n",
        "import torch.utils.data\n",
        "import math\n",
        "from skimage import io, transform\n",
        "from PIL import Image\n",
        "import torchvision as vision\n",
        "from torchvision import transforms, datasets\n",
        "# import random\n",
        "\n",
        "import torchvision.models as models\n",
        "\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "# For plotting\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Added for ResNet partitioning\n",
        "from collections import OrderedDict\n",
        "\n",
        "# For logfile\n",
        "import csv\n",
        "\n",
        "# For timing\n",
        "from datetime import datetime\n",
        "from pytz import timezone\n",
        "\n",
        "# Import fpzip for communication approximation\n",
        "import fpzip\n",
        "import pyzfp # import compress, decompress\n",
        "\n",
        "# Import random for random indices generation\n",
        "import random\n",
        "\n",
        "# Distiller import for compute approximation\n",
        "from distiller import models as dis_models\n",
        "import distiller\n",
        "from distiller.apputils import *\n",
        "\n",
        "# Determining device\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if train_on_gpu else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2KQxjS90SYu"
      },
      "outputs": [],
      "source": [
        "# Proper seed setting for reproducibility of the results\n",
        "# Taken from Slide 73 (DL)\n",
        "seed = 0\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.backends.cudnn.deterministic=True\n",
        "torch.backends.cudnn.benchmarks=False\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gg6F5ft9JYHe"
      },
      "outputs": [],
      "source": [
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6ixa-Jqz1sr"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi -L\n",
        "!cat /proc/meminfo\n",
        "# !/usr/local/cuda/bin/nvcc --version\n",
        "# !nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Khl8hD4Xr4UG"
      },
      "outputs": [],
      "source": [
        "print(os.cpu_count())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZYIBlWo6L-8"
      },
      "source": [
        "# Memory Approx Functions and classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndhSGpbI6Nit"
      },
      "outputs": [],
      "source": [
        "class MemoryApprox(object):\n",
        "  \"\"\"\n",
        "  Use already created error mask and perform bitwise xor\n",
        "  Args:\n",
        "  :param refresh_interval: DRAM refresh interval to select specific error mask\n",
        "  :param mask_dir: path to error mask\n",
        "  :param img_size: size of image, used to get mask of same size as image\n",
        "  :return: image injected with DRAM error in PIL format\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, refresh_interval: int, mask_dir: str, img_size):\n",
        "    self.refresh_interval = refresh_interval\n",
        "    # convert to single value if tuple provided, assumption is that the width and height are the same\n",
        "    if isinstance(img_size, tuple):\n",
        "      img_size = img_size[0]\n",
        "\n",
        "    if refresh_interval != 1:\n",
        "      # path to dram error mask which when XOR'ed with image will give error injected image\n",
        "      err_mask_root = os.path.join(mask_dir, 'mask_dram1_refint_{}_fm_2'.format(str(refresh_interval)))\n",
        "      err_mask_path = os.path.join(err_mask_root, 'error_mask_ri{}_{}'.format(refresh_interval, img_size))\n",
        "      # open error mask in image format and store in memory as this will be used for all images in dataset\n",
        "      self.error_mask = Image.open(err_mask_path)\n",
        "\n",
        "  def __call__(self, image):\n",
        "    if self.refresh_interval != 1:\n",
        "      # convert from PIL to numpy and do all operations\n",
        "      assert image.size == self.error_mask.size, \"Image size and Error mask size differs\"\n",
        "      # Perform bitwise xor of original resized and cropped image (assume coming from SENSOR directly)\n",
        "      # TODO: VERIFY IF CROP CAN BE DONE SEPARATELY AFTER DRAM ERROR\n",
        "      err_image_np = np.bitwise_xor(np.asarray(image), np.asarray(self.error_mask))\n",
        "\n",
        "      # convert from numpy.ndarray to PIL Image\n",
        "      image = TF.to_pil_image(err_image_np)\n",
        "    return image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUi-wSoD4bv-"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoXItnmx4WLB"
      },
      "outputs": [],
      "source": [
        "# Inherit Model from \"torch.nn.Module\"\n",
        "class Model(nn.Module):\n",
        "\n",
        "    def __init__(self, name):\n",
        "        super(Model, self).__init__()\n",
        "        self.name = name\n",
        "\n",
        "\n",
        "    # Function to save model\n",
        "    def save(self, path, epoch=0):\n",
        "        # Complete path to save the Model\n",
        "        complete_path = os.path.join(path, self.name)\n",
        "        if not os.path.exists(complete_path):\n",
        "            os.makedirs(complete_path)\n",
        "        # Uses PyTorch's torch.save to save the model every epoch\n",
        "        torch.save(self.state_dict(),\n",
        "                os.path.join(complete_path,\n",
        "                    \"model-{}.pth\".format(str(epoch).zfill(5))))\n",
        "\n",
        "\n",
        "    # Function to save results ??\n",
        "    # What is this function?\n",
        "    def save_results(self, path, data):\n",
        "        raise NotImplementedError(\"Model subclass must implement this method.\")\n",
        "\n",
        "\n",
        "    # Function to load Model\n",
        "    def load(self, path, modelfile=None):\n",
        "        # Get the path from where the saved model to be loaded\n",
        "        # complete_path = os.path.join(path, self.name)\n",
        "        complete_path = path\n",
        "        # If model does not exist, raise error\n",
        "        if not os.path.exists(complete_path):\n",
        "            raise IOError(\"{} directory does not exist in {}\".format(self.name, path))\n",
        "\n",
        "        # If no modelfile name is given\n",
        "        if modelfile is None:\n",
        "            # Grab the latest model file\n",
        "            model_files = glob.glob(complete_path+\"/*\")\n",
        "            mf = max(model_files)\n",
        "        else:\n",
        "            # Else grab the specific model file\n",
        "            mf = os.path.join(complete_path, modelfile)\n",
        "\n",
        "        # load that model file\n",
        "        # print(mf)\n",
        "        if device.type == 'cpu':\n",
        "          self.load_state_dict(torch.load(mf, map_location=torch.device('cpu')))\n",
        "        else:\n",
        "          self.load_state_dict(torch.load(mf))\n",
        "        # self.load_state_dict(torch.load(mf, map_location=torch.device('cpu')))\n",
        "        # self.load_state_dict(torch.load(mf), strict=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uQUf9aw4id4"
      },
      "source": [
        "# MVCNN and SVCNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ln9t5USPQXtk"
      },
      "outputs": [],
      "source": [
        "# from enum import unique\n",
        "# Modified MVCNN code for feature map compression\n",
        "\n",
        "def compress_tensor(input, method='zfp', compr_knob=2):\n",
        "  input_shape = input.shape\n",
        "  # print(input_shape)\n",
        "\n",
        "  # Get the original input size in bytes\n",
        "\n",
        "  # # Manual approach\n",
        "  # # It assumes that all calculations are done with float32\n",
        "  # original_size = 1\n",
        "  # # input_shape_list = list(input_shape)\n",
        "  # for element in input_shape:\n",
        "  #   original_size *= element\n",
        "  # original_size *= 4 # Get size in bytes\n",
        "\n",
        "  # Better approach\n",
        "  original_size = input.element_size() * input.nelement()\n",
        "\n",
        "  # Compression and decompression in numpy array\n",
        "  input = input.cpu() # Mode tensor to CPU for computation\n",
        "  data = input.detach().numpy()\n",
        "  # data = array(input)\n",
        "\n",
        "  # Compress and decompress (lossless or lossy)\n",
        "  if (method == 'fpzip'):\n",
        "    compressed_bytes = fpzip.compress(data, precision=compr_knob, order='C')\n",
        "    data_again = fpzip.decompress(compressed_bytes, order='C')\n",
        "  elif (method == 'zfp'):\n",
        "    parallel = True   # Not passed through function, local parameter, not frequently changed\n",
        "    compressed_bytes = pyzfp.compress(data, tolerance=compr_knob, parallel=parallel)\n",
        "    data_again = pyzfp.decompress(compressed_bytes, data.shape, data.dtype, tolerance=compr_knob)\n",
        "\n",
        "  # Convert data to tensor\n",
        "  decompressed_input = torch.tensor(data_again)\n",
        "  # print(decompressed_input)\n",
        "  # print(decompressed_input.shape)\n",
        "  decompressed_input = decompressed_input.view(input_shape)\n",
        "\n",
        "  # Sum of Absolute Differences\n",
        "  SAD = torch.sum(torch.abs(input - decompressed_input)).item()\n",
        "\n",
        "  # Send back to CUDA (if available)\n",
        "  decompressed_input = decompressed_input.to(device)\n",
        "\n",
        "  return decompressed_input, original_size, len(compressed_bytes), SAD\n",
        "\n",
        "def _get_non_dom_knob (inp_knob_list, knob_dom):\n",
        "  knob_list = inp_knob_list.copy()\n",
        "  if knob_dom in knob_list:\n",
        "    knob_list.remove(knob_dom)\n",
        "    if (len(knob_list) == 0):\n",
        "      return knob_dom\n",
        "    else:\n",
        "      return knob_list[0]\n",
        "  else:\n",
        "    print('Invalid dominant node knob')\n",
        "    return\n",
        "\n",
        "class MVCNN_approx_opt(Model):\n",
        "\n",
        "    def __init__(self, name, model, accurate_models_dict, nclasses=40, cnn_name='alexnet', num_views=12, part_point=5, compr_method = 'None', compr_knob=None, refresh_interval=[0,0,0,0,0,0,0,0,0,0,0,0], \\\n",
        "                 approx_model_dir='/content/drive/MyDrive/Arghadip/MVCNN/jongchyisu/Trained_Models/Approx_Mem', \\\n",
        "                 accurate_model_dir='/content/drive/MyDrive/Arghadip/MVCNN/jongchyisu/Trained_Models/Sensor_Subsampling_Baselines/w_gen_pool/epoch_15', \\\n",
        "                 sparsity=[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0], ri_dom=0, sp_dom=0.0,\\\n",
        "                 model_dom=None, model_nondom=None):\n",
        "        super(MVCNN_approx_opt, self).__init__(name)\n",
        "\n",
        "        self.classnames=['airplane','bathtub','bed','bench','bookshelf','bottle','bowl','car','chair',\n",
        "                         'cone','cup','curtain','desk','door','dresser','flower_pot','glass_box',\n",
        "                         'guitar','keyboard','lamp','laptop','mantel','monitor','night_stand',\n",
        "                         'person','piano','plant','radio','range_hood','sink','sofa','stairs',\n",
        "                         'stool','table','tent','toilet','tv_stand','vase','wardrobe','xbox']\n",
        "\n",
        "        self.nclasses = nclasses\n",
        "        self.num_views = num_views\n",
        "        self.part_point = part_point\n",
        "        self.compr_knob = compr_knob\n",
        "        self.compr_method = compr_method\n",
        "        self.feature_extractors = model.feature_extractors\n",
        "\n",
        "        # Get unique refresh interval list\n",
        "        self.refresh_interval = refresh_interval\n",
        "        self.approx_model_dir = approx_model_dir\n",
        "        self.accurate_model_dir = accurate_model_dir\n",
        "        self.accurate_models_dict = accurate_models_dict\n",
        "\n",
        "        unique_intervals = set(self.refresh_interval)\n",
        "        self.unique_intervals_list = list(unique_intervals)\n",
        "\n",
        "        # Unique sparsity list\n",
        "        self.sparsity = sparsity\n",
        "        unique_sparsities = set(self.sparsity)\n",
        "        self.unique_sparsities_list = list(unique_sparsities)\n",
        "\n",
        "        # Mean and standard deviation of the dataset used (NOT used anywhere)\n",
        "        self.mean = Variable(torch.FloatTensor([0.485, 0.456, 0.406]), requires_grad=False).to(device)\n",
        "        self.std = Variable(torch.FloatTensor([0.229, 0.224, 0.225]), requires_grad=False).to(device)\n",
        "\n",
        "        # use_resnet = True if \"cnn_name\" starts with \"resnet\"\n",
        "        self.use_resnet = cnn_name.startswith('resnet')\n",
        "\n",
        "        # Get the knob settings for dominant group\n",
        "        self.ri_dom = ri_dom\n",
        "        self.sp_dom = sp_dom\n",
        "\n",
        "        self.file_name = None\n",
        "\n",
        "        # # If \"resnet\"\n",
        "        # if self.use_resnet:\n",
        "        #     # # Get the layers before final classifier in \"net_1\" ??? VERIFY!!\n",
        "        #     # # Verified, pooling is done just before the final layer\n",
        "        #     # self.net_1 = nn.Sequential(*list(model.net.children())[:-1])\n",
        "        #     # # Get the final classifier in \"net_2\"\n",
        "        #     # self.net_2 = model.net.fc\n",
        "        #     self.net_1 = model.net_1\n",
        "        #     self.net_2 = model.net_2\n",
        "        #     self.net_3 = model.net_3\n",
        "\n",
        "        # # If NOT \"resnet\"\n",
        "        # else:\n",
        "        # Get 3 parts of the network\n",
        "        # self.net_1 = model.net_1\n",
        "        # self.net_2 = model.net_2\n",
        "        # self.net_3 = model.net_3\n",
        "\n",
        "        # path = '/content/drive/MyDrive/Arghadip/MVCNN/jongchyisu/Trained_Models/Sensor_Subsampling_Baselines/w_gen_pool/epoch_15'\n",
        "        # modelfile_mvcnn = 'Alexnet_MVCNN_pool_5_epoch_12.pth'\n",
        "        # accurate_models_dict = {\n",
        "        #           'alexnet_PP_5' : 'Alexnet_MVCNN_pool_5_epoch_12.pth',\n",
        "        #           'vgg11_PP_10'  : 'Vgg11_MVCNN_pool_10_epoch_12.pth',\n",
        "        #           'resnet34_PP_5': 'Resnet34_MVCNN_PP_5.pth'\n",
        "        # }\n",
        "\n",
        "\n",
        "        # If all views are working with equal knobs\n",
        "        if (len(self.unique_intervals_list) == 1) and (len(self.unique_sparsities_list) == 1):\n",
        "          # If accurate\n",
        "          if ((self.unique_intervals_list[0] == 0) or (self.unique_intervals_list[0] == 1)) and (self.unique_sparsities_list[0] == 0):\n",
        "            self.net_1 = model.net_1\n",
        "            self.net_2 = model.net_2\n",
        "            self.net_3 = model.net_3\n",
        "            self.load(self.accurate_model_dir, self.accurate_models_dict[cnn_name + '_PP_' + str(self.part_point)])\n",
        "            self.eval()\n",
        "            self.file_name = self.accurate_models_dict[cnn_name + '_PP_' + str(self.part_point)]\n",
        "          # else if approximate but all equal (load once)\n",
        "          else:\n",
        "            if ((self.unique_intervals_list[0] == 0) and (not (self.unique_sparsities_list[0] == 0))):\n",
        "              modelfile_mvcnn = 'approx_' + cnn_name + '_PP_' + str(part_point) + '_dram' + str(1) + '_sparsity_' + str(self.unique_sparsities_list[0]) + '.pth.tar'    # RI = 0 and 1 has no errors\n",
        "            else:\n",
        "              modelfile_mvcnn = 'approx_' + cnn_name + '_PP_' + str(part_point) + '_dram' + str(self.unique_intervals_list[0]) + '_sparsity_' + str(self.unique_sparsities_list[0]) + '.pth.tar'\n",
        "            # temp = distiller.models.create_model(pretrained=True, dataset='modelnet', arch= cnn_name + '_mvcnn', parallel=False, device_ids=-1)\n",
        "            # load_checkpoint(temp, os.path.join(self.approx_model_dir,modelfile_mvcnn))\n",
        "            # # print('Loaded dominant from ', modelfile_mvcnn)\n",
        "            # temp.eval()\n",
        "\n",
        "            temp = model_dom\n",
        "            self.file_name = modelfile_mvcnn\n",
        "            self.net_1 = temp.net_1\n",
        "            self.net_2 = temp.net_2\n",
        "            self.net_3 = temp.net_3\n",
        "        elif (len(self.unique_intervals_list) <= 2) and (len(self.unique_sparsities_list) <= 2):   # When operating with group-wise same knob settings\n",
        "          ri_nondom = _get_non_dom_knob (self.unique_intervals_list, self.ri_dom)\n",
        "          sp_nondom = _get_non_dom_knob (self.unique_sparsities_list, self.sp_dom)\n",
        "          modelfile1 = 'approx_' + cnn_name + '_PP_' + str(part_point) + '_dram' + str(self.ri_dom) + '_sparsity_' + str(self.sp_dom) + '.pth.tar'\n",
        "          modelfile2 = 'approx_' + cnn_name + '_PP_' + str(part_point) + '_dram' + str(ri_nondom) + '_sparsity_' + str(sp_nondom) + '.pth.tar'\n",
        "          self.file_name = modelfile1 + '_&_' + modelfile2\n",
        "          # temp = distiller.models.create_model(pretrained=True, dataset='modelnet', arch= cnn_name + '_mvcnn', parallel=False, device_ids=-1)\n",
        "\n",
        "          # # loading dominant model\n",
        "          # load_checkpoint(temp, os.path.join(self.approx_model_dir,modelfile1))\n",
        "          # # print('Loaded dominant from ', modelfile1)\n",
        "          # temp.eval()\n",
        "\n",
        "          temp = model_dom\n",
        "          if (self.use_resnet) or (self.part_point < self.feature_extractors):\n",
        "            self.net_1_dom = temp.net_1\n",
        "            self.net_2 = temp.net_2\n",
        "          else:\n",
        "            self.net_1_dom = temp.net_1\n",
        "            self.net_2_dom = temp.net_2\n",
        "          # self.net_3 = temp.net_3\n",
        "          del temp\n",
        "\n",
        "          # # loading non-dominant model\n",
        "          # temp2 = distiller.models.create_model(pretrained=True, dataset='modelnet', arch= cnn_name + '_mvcnn', parallel=False, device_ids=-1)\n",
        "          # load_checkpoint(temp2, os.path.join(self.approx_model_dir,modelfile2))\n",
        "          # # print('Loaded non-dominant from ', modelfile2)\n",
        "          # temp2.eval()\n",
        "\n",
        "          temp2 = model_nondom\n",
        "          if (self.use_resnet) or (self.part_point < self.feature_extractors):\n",
        "            self.net_1_nondom = temp2.net_1\n",
        "            self.net_2 = temp2.net_2\n",
        "          else:\n",
        "            self.net_1_nondom = temp2.net_1\n",
        "            self.net_2_nondom = temp2.net_2\n",
        "\n",
        "          self.net_3 = temp2.net_3     # no approximation is applied to net_3 (hence it does not matter from where it is loaded, it's same for all)\n",
        "          del temp2\n",
        "\n",
        "        else: # create the basic backbone to load later\n",
        "          modelfile_mvcnn = 'approx_' + cnn_name + '_PP_' + str(part_point) + '_dram' + str(self.unique_intervals_list[0]) + '_sparsity_' + str(self.unique_sparsities_list[0]) + '.pth.tar'\n",
        "          temp = distiller.models.create_model(pretrained=True, dataset='modelnet', arch= cnn_name + '_mvcnn', parallel=False, device_ids=-1)\n",
        "          load_checkpoint(temp, os.path.join(self.approx_model_dir,modelfile_mvcnn))\n",
        "          # print('Loaded non-dominant from ', modelfile_mvcnn)\n",
        "          temp.eval()\n",
        "          self.file_name = modelfile_mvcnn\n",
        "          self.net_1 = temp.net_1\n",
        "          self.net_2 = temp.net_2\n",
        "          self.net_3 = temp.net_3\n",
        "\n",
        "\n",
        "        # path = '/content/drive/MyDrive/Arghadip/MVCNN/Distiller/Compress_MVCNN/Alexnet/Logs/2022.06.18-223606_60_60_20'\n",
        "        # modelfile_mvcnn = 'checkpoint.pth.tar'\n",
        "\n",
        "        # if len(self.unique_intervals_list) == 1:\n",
        "        #   if not((self.unique_intervals_list[0] == 0) or (self.unique_intervals_list[0] == 1)):\n",
        "        #     path = self.approx_model_dir\n",
        "        #     modelfile_mvcnn = 'approx_' + cnn_name + '_PP_' + str(part_point) + '_dram' + str(self.unique_intervals_list[0]) + '.pth'\n",
        "\n",
        "        # # Load parameters\n",
        "        # # self.load(path, modelfile_mvcnn)\n",
        "        # # self.eval()\n",
        "\n",
        "        # # Load parameters for thinned model (distiller)\n",
        "        # # Load from distiller checkpoint (Approx Compute)\n",
        "        # # # Load thinned model from distiller\n",
        "        # temp = distiller.models.create_model(pretrained=True, dataset='modelnet', arch='alexnet_mvcnn', parallel=False, device_ids=-1)\n",
        "        # checkpoint_file = os.path.join(path,modelfile_mvcnn)\n",
        "        # load_checkpoint(temp, checkpoint_file)\n",
        "        # # checkpoint = torch.load(os.path.join(path,modelfile_mvcnn))\n",
        "        # # print(checkpoint)\n",
        "        # # model.load_state_dict(checkpoint['state_dict'])\n",
        "        # temp.eval()\n",
        "\n",
        "        # self.net_1 = temp.net_1\n",
        "        # self.net_2 = temp.net_2\n",
        "        # self.net_3 = temp.net_3\n",
        "\n",
        "    # Forward function defines how the inputs will pass through the network\n",
        "    def forward(self, x):\n",
        "      # if self.use_resnet:\n",
        "      #   # Pass the input through the \"net_1\"\n",
        "      #   y = self.net_1(x) # Intermediate output for all images in batch\n",
        "      #   # EXPLORE IN DETAILS!!\n",
        "      #   # Combine the inputs???? --> Explored!\n",
        "      #   # Group the feature maps corresponding to a single model\n",
        "      #   y = y.view((int(x.shape[0]/self.num_views),self.num_views,y.shape[-3],y.shape[-2],y.shape[-1]))#(8,12,512,7,7)\n",
        "      #   # Flatten and pass the pooled features to the final classifier\n",
        "      #   # Max pool among the feature maps of a single 3D model\n",
        "      #   return self.net_2(torch.max(y,1)[0].view(y.shape[0],-1))\n",
        "      # else:\n",
        "      batch_mem_orig = 0\n",
        "      batch_mem_compressed = 0\n",
        "      SAD_batch = 0\n",
        "\n",
        "      ####################### POOLING before FLATTENING #######################\n",
        "      #########################################################################\n",
        "      if (self.use_resnet) or (self.part_point < self.feature_extractors):\n",
        "\n",
        "        ################## Approximate Memory Mimic ##########################\n",
        "        if (len(self.unique_intervals_list) == 1) and (len(self.unique_sparsities_list) == 1): # No approx or same refresh interval for all views\n",
        "          y1 = self.net_1(x)\n",
        "          y1 = y1.view((int(x.shape[0]/self.num_views),self.num_views,y1.shape[-3],y1.shape[-2],y1.shape[-1]))\n",
        "        else: # More than one type of DRAM refresh intervals\n",
        "          x = x.view((int(x.shape[0]/self.num_views),self.num_views,x.shape[-3],x.shape[-2],x.shape[-1])) # Get the shape as (N,V,C,H,W)\n",
        "          x = torch.transpose(x,0,1)  # Transpose to get the shape as (V,N,C,H,W)\n",
        "          y1 = []\n",
        "          for i in range(self.num_views): # For each view\n",
        "\n",
        "            # # only approx memory\n",
        "            # net_1_file = 'approx_' + cnn_name + '_PP_' + str(part_point) + '_dram' + str(self.refresh_interval[i]) + '_net_1.pth'  # Get the 'net_1' parameter file for the RI\n",
        "            # # self.net_1.load(self.approx_model_dir,net_1_file)  # Load only 'net_1' params (running on EDGE)\n",
        "            # mf1 = os.path.join(self.approx_model_dir, net_1_file)\n",
        "            # if device.type == 'cpu':\n",
        "            #   self.net_1.load_state_dict(torch.load(mf1, map_location=torch.device('cpu')))\n",
        "            # else:\n",
        "            #   self.net_1.load_state_dict(torch.load(mf1))\n",
        "            # self.eval() # always eval() after loading\n",
        "\n",
        "            if (len(self.unique_intervals_list) <= 2) and (len(self.unique_sparsities_list) <= 2):\n",
        "              if (self.refresh_interval[i] == self.ri_dom) and (self.sparsity[i] == self.sp_dom):\n",
        "                # print('Passing though dominant, view = ',i+1, 'RI = ', self.refresh_interval[i], 'SP = ', self.sparsity[i])\n",
        "                y1_this_view = self.net_1_dom(x[i])\n",
        "              else:\n",
        "                y1_this_view = self.net_1_nondom(x[i])\n",
        "            else:\n",
        "              # with approx computing\n",
        "              modelfile_mvcnn = 'approx_' + cnn_name + '_PP_' + str(part_point) + '_dram' + str(self.refresh_interval[i]) + '_sparsity_' + str(self.sparsity[i]) + '.pth.tar'\n",
        "              temp = distiller.models.create_model(pretrained=True, dataset='modelnet', arch= cnn_name + '_mvcnn', parallel=False, device_ids=-1)\n",
        "              load_checkpoint(temp, os.path.join(self.approx_model_dir,modelfile_mvcnn))\n",
        "              temp.eval()\n",
        "              self.net_1 = temp.net_1\n",
        "              y1_this_view = self.net_1(x[i]) # Get the output of 'net_1' for this view\n",
        "\n",
        "            y1.append(y1_this_view) # Append to the list for all the views\n",
        "          y1 = torch.stack(y1)  # Convert the list to tensor\n",
        "          y1 = torch.transpose(y1,0,1)  # Transpose to get the shape as (N,V,C,H,W)\n",
        "\n",
        "\n",
        "        ################## Approximate communication mimic ####################\n",
        "        # If communication approximation needs to be applied\n",
        "        if self.compr_method != 'None':\n",
        "          y1 = torch.transpose(y1,0,1)  # Get V tensors of dimension (N,C,H,W) OR (V,N,C,H,W)\n",
        "          y1_compressed = []  # Empty list to append the view-wise compressed tensors of dimension (N,C,H,W)\n",
        "          for i in range(self.num_views):   # Start loop for number of views (as different views can have different level of compression)\n",
        "            # fpzip can work on a 4D tensor\n",
        "            if (self.compr_method == 'fpzip'):\n",
        "              # Returns compressed tensor with same shape as input, original tensor size and compressed tensor size in bytes, and SAD of input and output\n",
        "              view_compressed, original_size, compressed_size, SAD = compress_tensor(y1[i],'fpzip', self.compr_knob[i])\n",
        "            # zfp works with 3D tensor (needs additional loop)\n",
        "            elif (self.compr_method == 'zfp'):\n",
        "              view_compressed = []\n",
        "              original_size = 0\n",
        "              compressed_size = 0\n",
        "              SAD = 0\n",
        "              # Start loop for each channel of each view\n",
        "              for j in range(y1[i].shape[0]):\n",
        "                channel_compressed, orig_ch_size, compr_ch_size, SAD_ch = compress_tensor(y1[i][j],'zfp', self.compr_knob[i])\n",
        "                # Append and increment\n",
        "                view_compressed.append(channel_compressed)\n",
        "                original_size += orig_ch_size\n",
        "                compressed_size += compr_ch_size\n",
        "                SAD += SAD_ch\n",
        "              view_compressed = torch.stack(view_compressed)  # Convert to tensor\n",
        "            y1_compressed.append(view_compressed) # Append to form the entire batch's FM\n",
        "            batch_mem_orig += original_size # Increment byte count for original tensor\n",
        "            batch_mem_compressed += compressed_size # Increment byte count for the compressed tensor\n",
        "            SAD_batch += SAD # Increment the SAD value\n",
        "          y1_compressed = torch.stack(y1_compressed)  # Cast the list to tensor\n",
        "          y1 = torch.transpose(y1_compressed,0,1) # Take transpose to get back the original shape (N,V,C,H,W)\n",
        "\n",
        "        y2 = self.net_2(torch.max(y1,1)[0])   # MAX pooling\n",
        "        # y2 = self.net_2(torch.mean(y1,1))     # AVG pooling\n",
        "        return self.net_3(y2.view(y2.shape[0],-1)), batch_mem_orig, batch_mem_compressed, SAD_batch\n",
        "\n",
        "      ####################### POOLING after FLATTENING ########################\n",
        "      #########################################################################\n",
        "      else:\n",
        "\n",
        "        ################## Approximate Memory Mimic ##########################\n",
        "        if (len(self.unique_intervals_list) == 1) and (len(self.unique_sparsities_list) == 1): # No approx or same refresh interval for all views\n",
        "          y1 = self.net_1(x)\n",
        "          y2 = self.net_2(y1.view(y1.shape[0],-1))\n",
        "          y2 = y2.view((int(x.shape[0]/self.num_views),self.num_views,y2.shape[-1]))\n",
        "        else: # More than one type of DRAM refresh intervals\n",
        "          x = x.view((int(x.shape[0]/self.num_views),self.num_views,x.shape[-3],x.shape[-2],x.shape[-1])) # Get the shape as (N,V,C,H,W)\n",
        "          x = torch.transpose(x,0,1)  # Transpose to get the shape as (V,N,C,H,W)\n",
        "          y2 = []\n",
        "          for i in range(self.num_views): # For each view\n",
        "\n",
        "            # # Only approx memory\n",
        "            # net_1_file = 'approx_' + cnn_name + '_PP_' + str(part_point) + '_dram' + str(self.refresh_interval[i]) + '_net_1.pth'  # Get the 'net_1' parameter file for the RI\n",
        "            # net_2_file = 'approx_' + cnn_name + '_PP_' + str(part_point) + '_dram' + str(self.refresh_interval[i]) + '_net_2.pth'  # Get the 'net_2' parameter file for the RI\n",
        "            # # self.net_1.load(self.approx_model_dir,net_1_file)  # Load 'net_1' params (running on EDGE)\n",
        "            # # self.net_2.load(self.approx_model_dir,net_2_file)  # Load 'net_2' params (running on EDGE)\n",
        "            # mf1 = os.path.join(self.approx_model_dir, net_1_file)\n",
        "            # mf2 = os.path.join(self.approx_model_dir, net_2_file)\n",
        "            # if device.type == 'cpu':\n",
        "            #   self.net_1.load_state_dict(torch.load(mf1, map_location=torch.device('cpu')))\n",
        "            #   self.net_2.load_state_dict(torch.load(mf2, map_location=torch.device('cpu')))\n",
        "            # else:\n",
        "            #   self.net_1.load_state_dict(torch.load(mf1))\n",
        "            #   self.net_2.load_state_dict(torch.load(mf2))\n",
        "            # self.eval() # always eval() after loading\n",
        "\n",
        "            if (len(self.unique_intervals_list) <= 2) and (len(self.unique_sparsities_list) <= 2):\n",
        "              if (self.refresh_interval[i] == self.ri_dom) and (self.sparsity[i] == self.sp_dom):\n",
        "                # print('Passing though dominant, view = ',i+1, 'RI = ', self.refresh_interval[i], 'SP = ', self.sparsity[i])\n",
        "                y1_this_view = self.net_1_dom(x[i])\n",
        "                y2_this_view = self.net_2_dom(y1_this_view.view(y1_this_view.shape[0],-1))  # Flattened and fed to 'net_2'\n",
        "              else:\n",
        "                y1_this_view = self.net_1_nondom(x[i])\n",
        "                y2_this_view = self.net_2_nondom(y1_this_view.view(y1_this_view.shape[0],-1))  # Flattened and fed to 'net_2'\n",
        "            else:\n",
        "              # with approx computing\n",
        "              modelfile_mvcnn = 'approx_' + cnn_name + '_PP_' + str(part_point) + '_dram' + str(self.refresh_interval[i]) + '_sparsity_' + str(self.sparsity[i]) + '.pth.tar'\n",
        "              temp = distiller.models.create_model(pretrained=True, dataset='modelnet', arch= cnn_name + '_mvcnn', parallel=False, device_ids=-1)\n",
        "              load_checkpoint(temp, os.path.join(self.approx_model_dir,modelfile_mvcnn))\n",
        "              temp.eval()\n",
        "              self.net_1 = temp.net_1\n",
        "              self.net_2 = temp.net_2\n",
        "              y1_this_view = self.net_1(x[i]) # Get the output of 'net_1' for this view\n",
        "              y2_this_view = self.net_2(y1_this_view.view(y1_this_view.shape[0],-1))  # Flattened and fed to 'net_2'\n",
        "\n",
        "            y2.append(y2_this_view) # Append to the list for all the views\n",
        "          y2 = torch.stack(y2)  # Convert the list to tensor (V,N,C*H*W)\n",
        "          y2 = torch.transpose(y2,0,1)  # Transpose to get the shape as (N,V,C*H*W)\n",
        "\n",
        "\n",
        "        ################## Approximate communication mimic ####################\n",
        "        # If communication approximation needs to be applied\n",
        "        if self.compr_method != 'None':\n",
        "          y2 = torch.transpose(y2,0,1)  # Get V tensors of dimension (N,C*H*W) OR (V,N,C*H*W)\n",
        "          y2_compressed = []  # Empty list to append the view-wise compressed tensors of dimension (N,C*H*W)\n",
        "          for i in range(self.num_views):   # Start loop for number of views (as different views can have different level of compression)\n",
        "            if (self.compr_method == 'fpzip'):\n",
        "              # Returns compressed tensor with same shape as input, original tensor size and compressed tensor size in bytes, and SAD of input and output\n",
        "              view_compressed, original_size, compressed_size, SAD = compress_tensor(y2[i],'fpzip', self.compr_knob[i])\n",
        "            elif (self.compr_method == 'zfp'):\n",
        "              # Returns compressed tensor with same shape as input, original tensor size and compressed tensor size in bytes, and SAD of input and output\n",
        "              view_compressed, original_size, compressed_size, SAD = compress_tensor(y2[i],'zfp', self.compr_knob[i])\n",
        "            y2_compressed.append(view_compressed) # Append to form the entire batch's FM\n",
        "            batch_mem_orig += original_size # Increment byte count for original tensor\n",
        "            batch_mem_compressed += compressed_size # Increment byte count for the compressed tensor\n",
        "            SAD_batch += SAD # Increment the SAD value\n",
        "          y2_compressed = torch.stack(y2_compressed)  # Cast the list to tensor\n",
        "          y2 = torch.transpose(y2_compressed,0,1) # Take transpose to get back the original shape (N,V,C*H*W)\n",
        "\n",
        "        return self.net_3(torch.max(y2,1)[0]), batch_mem_orig, batch_mem_compressed, SAD_batch   # MAX pooling\n",
        "        # return self.net_3(torch.mean(y2,1)), batch_mem_orig, batch_mem_compressed, SAD_batch   # AVG pooling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OV5asv9N4bFA"
      },
      "outputs": [],
      "source": [
        "# Mean and standard deviation of the dataset (Derived from ModelNet40)\n",
        "mean = Variable(torch.FloatTensor([0.485, 0.456, 0.406]), requires_grad=False).to(device)\n",
        "std = Variable(torch.FloatTensor([0.229, 0.224, 0.225]), requires_grad=False).to(device)\n",
        "\n",
        "# Definition of flip function\n",
        "# What it does?\n",
        "def flip(x, dim):\n",
        "    xsize = x.size()\n",
        "    dim = x.dim() + dim if dim < 0 else dim\n",
        "    x = x.view(-1, *xsize[dim:])\n",
        "    x = x.view(x.size(0), x.size(1), -1)[:, getattr(torch.arange(x.size(1)-1,\n",
        "                      -1, -1), ('cpu','cuda')[x.is_cuda])().long(), :]\n",
        "    return x.view(xsize)\n",
        "\n",
        "\n",
        "class SVCNN(Model):\n",
        "\n",
        "    def __init__(self, name, nclasses=40, pretraining=True, cnn_name='vgg11', part_point=5):\n",
        "        super(SVCNN, self).__init__(name)\n",
        "        # \"name\" is a dummy parameter\n",
        "\n",
        "        # All the classes in ModelNet40\n",
        "        self.classnames=['airplane','bathtub','bed','bench','bookshelf','bottle','bowl','car','chair',\n",
        "                         'cone','cup','curtain','desk','door','dresser','flower_pot','glass_box',\n",
        "                         'guitar','keyboard','lamp','laptop','mantel','monitor','night_stand',\n",
        "                         'person','piano','plant','radio','range_hood','sink','sofa','stairs',\n",
        "                         'stool','table','tent','toilet','tv_stand','vase','wardrobe','xbox']\n",
        "\n",
        "        self.nclasses = nclasses\n",
        "        self.pretraining = pretraining\n",
        "        self.cnn_name = cnn_name\n",
        "        self.part_point = part_point\n",
        "\n",
        "        # use_resnet = True if \"cnn_name\" starts with \"resnet\"\n",
        "        self.use_resnet = cnn_name.startswith('resnet')\n",
        "\n",
        "        # HARDCODING of the inherent division of the CNN (feature extractor and classifier)\n",
        "        # Check values and update manually\n",
        "        if not self.use_resnet:\n",
        "          if self.cnn_name == 'alexnet':\n",
        "            self.feature_extractors = 12\n",
        "          elif self.cnn_name == 'vgg11':\n",
        "            self.feature_extractors = 20\n",
        "          elif self.cnn_name == 'vgg16':\n",
        "            self.feature_extractors = 30\n",
        "        else:\n",
        "          self.feature_extractors = 8\n",
        "          # if self.cnn_name == 'resnet18':\n",
        "          #   self.feature_extractors = 999\n",
        "          # if self.cnn_name == 'resnet34':\n",
        "          #   self.feature_extractors = 999\n",
        "          # if self.cnn_name == 'resnet50':\n",
        "          #   self.feature_extractors = 999\n",
        "\n",
        "        # Mean and standard deviation of the dataset used (NOT used anywhere)\n",
        "        self.mean = Variable(torch.FloatTensor([0.485, 0.456, 0.406]), requires_grad=False).to(device)\n",
        "        self.std = Variable(torch.FloatTensor([0.229, 0.224, 0.225]), requires_grad=False).to(device)\n",
        "\n",
        "        # If \"resnet\"\n",
        "        if self.use_resnet:\n",
        "          if self.cnn_name == 'resnet18':\n",
        "              # Load the model from PyTorch repo\n",
        "              # Set the \"pretrained\" value based on the passed argument\n",
        "              self.net = models.resnet18(pretrained=self.pretraining)\n",
        "              # Change the input and output dimensions of the final classifier (fc) layer\n",
        "              # 40 classes in ModelNet40\n",
        "              self.net.fc = nn.Linear(512,40)\n",
        "          elif self.cnn_name == 'resnet34':\n",
        "              self.net = models.resnet34(pretrained=self.pretraining)\n",
        "              self.net.fc = nn.Linear(512,40)\n",
        "          elif self.cnn_name == 'resnet50':\n",
        "              self.net = models.resnet50(pretrained=self.pretraining)\n",
        "              self.net.fc = nn.Linear(2048,40)\n",
        "\n",
        "          self.resnet_modules = [\n",
        "              ('conv1',self.net.conv1),\n",
        "              ('bn1', self.net.bn1),\n",
        "              ('relu', self.net.relu),\n",
        "              ('maxpool', self.net.maxpool),\n",
        "              ('layer1', self.net.layer1),\n",
        "              ('layer2', self.net.layer2),\n",
        "              ('layer3', self.net.layer3),\n",
        "              ('layer4', self.net.layer4),\n",
        "              ('avgpool', self.net.avgpool)\n",
        "          ]\n",
        "          # Partitioning\n",
        "          if (self.part_point < self.feature_extractors):\n",
        "            self.net_1 = nn.Sequential(OrderedDict(self.resnet_modules[0:self.part_point+1]))\n",
        "            self.net_2 = nn.Sequential(OrderedDict(self.resnet_modules[self.part_point+1:]))\n",
        "          else:\n",
        "            self.net_1 = nn.Sequential(OrderedDict(self.resnet_modules[0:self.feature_extractors+1]))\n",
        "            self.net_2 = nn.Sequential(OrderedDict(self.resnet_modules[self.feature_extractors:self.feature_extractors]))\n",
        "          self.net_3 = self.net.fc\n",
        "            # if self.cnn_name == 'resnet18':\n",
        "            #     # Load the model from PyTorch repo\n",
        "            #     # Set the \"pretrained\" value based on the passed argument\n",
        "            #     self.net = models.resnet18(pretrained=self.pretraining)\n",
        "            #     # Change the input and output dimensions of the final classifier (fc) layer\n",
        "            #     # 40 classes in ModelNet40\n",
        "            #     self.net.fc = nn.Linear(512,40)\n",
        "            # elif self.cnn_name == 'resnet34':\n",
        "            #     self.net = models.resnet34(pretrained=self.pretraining)\n",
        "            #     self.net.fc = nn.Linear(512,40)\n",
        "            # elif self.cnn_name == 'resnet50':\n",
        "            #     self.net = models.resnet50(pretrained=self.pretraining)\n",
        "            #     self.net.fc = nn.Linear(2048,40)\n",
        "\n",
        "        # If NOT resnet\n",
        "        # \"net_1\" = Feature Extractor\n",
        "        # \"net_2\" = Classifier\n",
        "        else:\n",
        "          if (self.part_point < self.feature_extractors):\n",
        "            if self.cnn_name == 'alexnet':\n",
        "                self.net_1 = models.alexnet(pretrained=self.pretraining).features[0:self.part_point+1]\n",
        "                self.net_2 = models.alexnet(pretrained=self.pretraining).features[self.part_point+1:self.feature_extractors+1]\n",
        "                self.net_3 = models.alexnet(pretrained=self.pretraining).classifier\n",
        "            elif self.cnn_name == 'vgg11':\n",
        "                self.net_1 = models.vgg11(pretrained=self.pretraining).features[0:self.part_point+1]\n",
        "                self.net_2 = models.vgg11(pretrained=self.pretraining).features[self.part_point+1:self.feature_extractors+1]\n",
        "                self.net_3 = models.vgg11(pretrained=self.pretraining).classifier\n",
        "            elif self.cnn_name == 'vgg16':\n",
        "                self.net_1 = models.vgg16(pretrained=self.pretraining).features[0:self.part_point+1]\n",
        "                self.net_2 = models.vgg16(pretrained=self.pretraining).features[self.part_point+1:self.feature_extractors+1]\n",
        "                self.net_3 = models.vgg16(pretrained=self.pretraining).classifier\n",
        "\n",
        "            # self.net_2._modules['6'] = nn.Linear(4096,40)\n",
        "            # self.net_2._modules['1'][6] = nn.Linear(4096,40)  # Changed for partitioning change\n",
        "            # self.net_3._modules['6'] = nn.Linear(4096,40)\n",
        "          else:\n",
        "            if self.cnn_name == 'alexnet':\n",
        "                self.net_1 = models.alexnet(pretrained=self.pretraining).features\n",
        "                self.net_2 = models.alexnet(pretrained=self.pretraining).classifier[0:min((self.part_point-self.feature_extractors),6)]\n",
        "                self.net_3 = models.alexnet(pretrained=self.pretraining).classifier[min((self.part_point-self.feature_extractors),6):]\n",
        "            elif self.cnn_name == 'vgg11':\n",
        "                self.net_1 = models.vgg11(pretrained=self.pretraining).features\n",
        "                self.net_2 = models.vgg11(pretrained=self.pretraining).classifier[0:min((self.part_point-self.feature_extractors),6)]\n",
        "                self.net_3 = models.vgg11(pretrained=self.pretraining).classifier[min((self.part_point-self.feature_extractors),6):]\n",
        "            elif self.cnn_name == 'vgg16':\n",
        "                self.net_1 = models.vgg16(pretrained=self.pretraining).features\n",
        "                self.net_2 = models.vgg16(pretrained=self.pretraining).classifier[0:min((self.part_point-self.feature_extractors),6)]\n",
        "                self.net_3 = models.vgg16(pretrained=self.pretraining).classifier[min((self.part_point-self.feature_extractors),6):]\n",
        "          # Outside if-else\n",
        "          self.net_3._modules['6'] = nn.Linear(4096,40)\n",
        "\n",
        "\n",
        "\n",
        "    # Forward function for the inputs\n",
        "    def forward(self, x):\n",
        "        # If resnet, just pass the input through the entire netowrk\n",
        "        # if self.use_resnet:\n",
        "        #     # return self.net(x)\n",
        "        #     y1 = self.net_1(x)\n",
        "        #     y2 = self.net_2(y1)\n",
        "        #     return self.net_3(y2.view(y2.shape[0],-1))\n",
        "        # # If NOT resnet\n",
        "        # # Store the extracted features in \"y\"\n",
        "        # # Then pass the modified flattened \"y\" to the classifier\n",
        "        # else:\n",
        "        if (self.use_resnet) or (self.part_point < self.feature_extractors):\n",
        "          # Pass through net_1\n",
        "          y1 = self.net_1(x)\n",
        "          # Pass through net_2\n",
        "          y2 = self.net_2(y1)\n",
        "          # Retain the first dimension of y i.e num_classes (=40)\n",
        "          # (-1) in torch.view() automatically get the 2nd dimension by merging the other dimensions\n",
        "          # For example, if original shape of y is [40 256, 6, 6], then after applying this, the shape\n",
        "          # will be [40, 9216]\n",
        "          return self.net_3(y2.view(y2.shape[0],-1))\n",
        "        else:\n",
        "          # Pass through net_1\n",
        "          y1 = self.net_1(x)\n",
        "          # Flatten y1 and Pass through net_2\n",
        "          y2 = self.net_2(y1.view(y1.shape[0],-1))\n",
        "          # Pass through net_3\n",
        "          return self.net_3(y2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Batchwise MultiImgDataset"
      ],
      "metadata": {
        "id": "n7joxKV5mdBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load multiview dataset\n",
        "class Multiview_Dataset_Batch(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, root_dir, scale_aug=False, rot_aug=False, test_mode=True, \\\n",
        "                 num_models=0, num_views=12, shuffle=False, subsampling_factor = [1,1,1,1,1,1,1,1,1,1,1,1], refresh_interval=[1,1,1,1,1,1,1,1,1,1,1,1], \\\n",
        "                 mask_dir='/content/drive/MyDrive/Arghadip/MVCNN/AxIS_ref/Approx_DRAM/error_mask',interpolation=Image.NEAREST, \\\n",
        "                 start_idx=0, end_idx=2468):\n",
        "        # Available classes in ModelNet40 dataset\n",
        "        self.classnames=['airplane','bathtub','bed','bench','bookshelf','bottle','bowl','car','chair',\n",
        "                         'cone','cup','curtain','desk','door','dresser','flower_pot','glass_box',\n",
        "                         'guitar','keyboard','lamp','laptop','mantel','monitor','night_stand',\n",
        "                         'person','piano','plant','radio','range_hood','sink','sofa','stairs',\n",
        "                         'stool','table','tent','toilet','tv_stand','vase','wardrobe','xbox']\n",
        "        # Get the arguments\n",
        "        self.root_dir = root_dir\n",
        "        self.scale_aug = scale_aug\n",
        "        self.rot_aug = rot_aug\n",
        "        self.test_mode = test_mode\n",
        "        self.num_views = num_views\n",
        "        # self.image_size = round(224/subsampling_factor)\n",
        "        self.image_size = [round(224/ssf) for ssf in subsampling_factor]\n",
        "        self.interpolation = interpolation\n",
        "\n",
        "        # Approx Memory\n",
        "        self.refresh_interval = refresh_interval\n",
        "        self.mask_dir = mask_dir\n",
        "\n",
        "        # \"set_\" determines the \"train\" or \"test\"\n",
        "        set_ = root_dir.split('/')[-1]\n",
        "        # Hold the name of the parent directory \"modelnet40_images_new_12x\"\n",
        "        parent_dir = root_dir.rsplit('/',2)[0]\n",
        "        self.filepaths = []\n",
        "        self.num_models_per_category = []\n",
        "        # Start loop for all the classes in the dataset\n",
        "        for i in range(len(self.classnames)):\n",
        "            # The glob module finds all the pathnames matching a specified pattern according to the rules\n",
        "            # And then sort all the pathnames\n",
        "            all_files = sorted(glob.glob(parent_dir+'/'+self.classnames[i]+'/'+set_+'/*.png'))\n",
        "            # Changed from .png to .off\n",
        "            # all_files = sorted(glob.glob(parent_dir+'/'+self.classnames[i]+'/'+set_+'/*.off'))\n",
        "\n",
        "            ## Select subset for different number of views (existing comment)\n",
        "            # Get the number of files to skip for a view from specific angle???\n",
        "            stride = int(12/self.num_views) # 12 6 4 3 2 1\n",
        "            # Skip the \"stride\" number of files for subset of views\n",
        "            all_files = all_files[::stride]\n",
        "\n",
        "            # There are 1000 iamges for each class\n",
        "\n",
        "            # self.num_models_per_category.append(int(len(all_files)/num_views))\n",
        "\n",
        "             # If there is no \"model\" for all classes (emtpty directory)\n",
        "            if num_models == 0:\n",
        "                # Use the whole dataset\n",
        "                self.filepaths.extend(all_files)\n",
        "                self.num_models_per_category.append(int(len(all_files)/num_views))\n",
        "            else:\n",
        "                # Use the \"num_models\" number of files\n",
        "                self.filepaths.extend(all_files[:min(num_models,len(all_files))])\n",
        "                # print(len(all_files[:min(num_models,len(all_files))]))\n",
        "                self.num_models_per_category.append(int(len(all_files[:min(num_models,len(all_files))])/num_views))\n",
        "\n",
        "            # ################### Log the number of models per category ##########\n",
        "            # cat_dict = {\n",
        "            #     'Category': self.classnames[i],\n",
        "            #     'Cat_idx' : i,\n",
        "            #     'Num_models': int(len(all_files)/num_views)\n",
        "            # }\n",
        "            # fields = cat_dict.keys()\n",
        "            # logfile = '/content/data/models_per_category.csv'\n",
        "\n",
        "            # with open(logfile, 'a') as csvfile:\n",
        "            #   csvwriter = csv.DictWriter(csvfile, fieldnames=fields)\n",
        "            #   if (i == 0):\n",
        "            #     csvwriter.writeheader()\n",
        "            #   csvwriter.writerow(cat_dict)\n",
        "            # csvfile.close()\n",
        "\n",
        "        if shuffle==True:\n",
        "            # permute\n",
        "            rand_idx = np.random.permutation(int(len(self.filepaths)/num_views))\n",
        "            filepaths_new = []\n",
        "            for i in range(len(rand_idx)):\n",
        "                filepaths_new.extend(self.filepaths[rand_idx[i]*num_views:(rand_idx[i]+1)*num_views])\n",
        "            self.filepaths = filepaths_new\n",
        "\n",
        "        self.filepaths = self.filepaths[start_idx*num_views:end_idx*num_views]\n",
        "\n",
        "\n",
        "        # If \"test_mode=False\"\n",
        "        if not self.test_mode:\n",
        "            # Ranomly flip the images horizontally\n",
        "            # Convert the image to tensor and then normalize\n",
        "            self.transform = transforms.Compose([\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                     std=[0.229, 0.224, 0.225])\n",
        "            ])\n",
        "\n",
        "    # Utility function to get the number of elements in the dataset\n",
        "    def __len__(self):\n",
        "        return int(len(self.filepaths)/self.num_views)\n",
        "\n",
        "    # Function to get \"items\" (all the views for a particular model) from the dataset\n",
        "    def __getitem__(self, idx):\n",
        "        # Get the \"filepath\" for the particular index\n",
        "        path = self.filepaths[idx*self.num_views]\n",
        "        # Get the \"class name\" by splitting the path and then picking the proper element\n",
        "        class_name = path.split('/')[-3]\n",
        "        # Get the integer index of the \"class name\"\n",
        "        class_id = self.classnames.index(class_name)\n",
        "        # Use PIL instead (existing comment)\n",
        "        imgs = []\n",
        "        # Start the loop for all the views\n",
        "        for i in range(self.num_views):\n",
        "            # Get the 2D image of the \"i\"-th view of the \"idx\"-th 3D object\n",
        "            # Convert the image to RGB format\n",
        "            im = Image.open(self.filepaths[idx*self.num_views+i]).convert('RGB')\n",
        "            if self.test_mode:\n",
        "              # print('Applied sensor subsampling, image size = ',self.image_size, ', interpolation = ', self.interpolation)\n",
        "              # Downsample, then upsample: Mimics the effect of sensor subsampling\n",
        "              self.transform = transforms.Compose([\n",
        "                  # OLD (BILINEAR ALWAYS)\n",
        "                  # transforms.Resize(self.image_size[i], interpolation=self.interpolation),   # Downsample\n",
        "                  # transforms.Resize(224), # Upsample back to 224, don't put this line after ToTensor, doesn't mimic sensor subsampling (more data loss)\n",
        "\n",
        "                  # NEW\n",
        "                  transforms.Resize(self.image_size[i], interpolation=self.interpolation),   # Downsample\n",
        "                  transforms.Resize(224, interpolation=self.interpolation), # Upsample back to 224, don't put this line after ToTensor, doesn't mimic sensor subsampling (more data loss)\n",
        "\n",
        "                  MemoryApprox(self.refresh_interval[i], self.mask_dir, 224), # Approx Memory on image\n",
        "                  transforms.ToTensor(),\n",
        "                  transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                      std=[0.229, 0.224, 0.225])\n",
        "              ])\n",
        "\n",
        "            # Apply the transform on the image (Pre-processing)\n",
        "            if self.transform:\n",
        "              # print('Applied sensor subsampling transform!')\n",
        "              im = self.transform(im)\n",
        "            # Append the tensor \"im\" to the \"imgs\"\n",
        "            imgs.append(im)\n",
        "\n",
        "        # Return class_id, \"num_views\" number of tensors and filepaths\n",
        "        return (class_id, torch.stack(imgs), self.filepaths[idx*self.num_views:(idx+1)*self.num_views])\n",
        "\n",
        "### Sampler to load a batch with models from a particular class only\n",
        "class DRAX_Classwise_Batch_Sampler(torch.utils.data.BatchSampler):\n",
        "    def __init__(self, class_counts, batch_size):\n",
        "        self.class_counts = class_counts\n",
        "        self.batch_size = batch_size\n",
        "        self.indices = list(range(sum(self.class_counts)))\n",
        "        self.batch_sizes = self._genrate_batch_sizes()\n",
        "\n",
        "    def _genrate_batch_sizes(self):\n",
        "        batch_sizes = []\n",
        "        for class_count in self.class_counts:\n",
        "          num_batches = int(class_count / self.batch_size)\n",
        "          remaining_models = int(class_count % self.batch_size)\n",
        "          cur_batch_size = [self.batch_size]*num_batches\n",
        "          cur_batch_size.extend([remaining_models])\n",
        "          batch_sizes.extend(cur_batch_size)\n",
        "        ## Filter out 0 indices\n",
        "        batch_sizes = [x for x in batch_sizes if x != 0]\n",
        "        return batch_sizes\n",
        "\n",
        "\n",
        "    def __iter__(self):\n",
        "        batch_start = 0\n",
        "        for batch_size in self.batch_sizes:\n",
        "            batch_indices = self.indices[batch_start:batch_start + batch_size]\n",
        "            yield batch_indices\n",
        "            batch_start += batch_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.batch_sizes)"
      ],
      "metadata": {
        "id": "F-INYFA8mi7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkANxBRV42QX"
      },
      "source": [
        "# SingleImgDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYfWVTxP41qf"
      },
      "outputs": [],
      "source": [
        "# Load singleview dataset\n",
        "# Treats all the images as individual, i.e. different views of a single 3D object is treated as independent images\n",
        "class SingleImgDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, root_dir, scale_aug=False, rot_aug=False, test_mode=False, \\\n",
        "                 num_models=0, num_views=12):\n",
        "        # \"num_views\" is NOT used\n",
        "        self.classnames=['airplane','bathtub','bed','bench','bookshelf','bottle','bowl','car','chair',\n",
        "                         'cone','cup','curtain','desk','door','dresser','flower_pot','glass_box',\n",
        "                         'guitar','keyboard','lamp','laptop','mantel','monitor','night_stand',\n",
        "                         'person','piano','plant','radio','range_hood','sink','sofa','stairs',\n",
        "                         'stool','table','tent','toilet','tv_stand','vase','wardrobe','xbox']\n",
        "        self.root_dir = root_dir\n",
        "        self.scale_aug = scale_aug\n",
        "        self.rot_aug = rot_aug\n",
        "        self.test_mode = test_mode  # NOT USED, dummy argument\n",
        "\n",
        "        set_ = root_dir.split('/')[-1]\n",
        "        parent_dir = root_dir.rsplit('/',2)[0]\n",
        "        self.filepaths = []\n",
        "        for i in range(len(self.classnames)):\n",
        "            all_files = sorted(glob.glob(parent_dir+'/'+self.classnames[i]+'/'+set_+'/*shaded*.png'))\n",
        "            # Changed from .png to .off\n",
        "            # all_files = sorted(glob.glob(parent_dir+'/'+self.classnames[i]+'/'+set_+'/*.off'))\n",
        "            if num_models == 0:\n",
        "                # Use the whole dataset\n",
        "                self.filepaths.extend(all_files)\n",
        "            else:\n",
        "                self.filepaths.extend(all_files[:min(num_models,len(all_files))])\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    # Gets the length of the dataset\n",
        "    def __len__(self):\n",
        "        return len(self.filepaths)\n",
        "\n",
        "    # Function to get \"item\" from the dataset\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.filepaths[idx]\n",
        "        class_name = path.split('/')[-3]\n",
        "        class_id = self.classnames.index(class_name)\n",
        "\n",
        "        # Use PIL instead\n",
        "        im = Image.open(self.filepaths[idx]).convert('RGB')\n",
        "        if self.transform:\n",
        "            im = self.transform(im)\n",
        "\n",
        "        return (class_id, im, path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0oJorClR41w"
      },
      "source": [
        "# Logging Helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTPaCtuT7Ros"
      },
      "outputs": [],
      "source": [
        "# Generate the group-wise knobs based significance\n",
        "def generate_groupwise_knobs (num_views, d, knob_dom, knob_non_dom, significance, mode='best'):\n",
        "  knobs = []\n",
        "  # Get the max indices based on significance of nodes\n",
        "  dom_indices = []\n",
        "  if 0 < d < num_views:\n",
        "    if (mode == 'best'):\n",
        "      value_to_search = 13\n",
        "      for i in range(d):\n",
        "        value_to_search -= 1\n",
        "        dom_indices.append(significance.index(value_to_search))\n",
        "    elif (mode == 'worst'):\n",
        "      value_to_search = 0\n",
        "      for i in range(d):\n",
        "        value_to_search += 1\n",
        "        dom_indices.append(significance.index(value_to_search))\n",
        "    elif (mode == 'alternate'):\n",
        "      for i in range(d):\n",
        "        dom_indices.append(min(i*2, num_views-1))\n",
        "    elif (mode == 'random'):\n",
        "      random.seed(seed)\n",
        "      list_indices = [i for i in range(num_views)]\n",
        "      random.shuffle(list_indices)\n",
        "      for i in range(d):\n",
        "        # temp = random.choice(list_indices)\n",
        "        temp = list_indices[i]\n",
        "        dom_indices.append(temp)\n",
        "        list_indices.remove(temp)\n",
        "    else:\n",
        "      dom_indices = [i for i in range(num_views)]   # All dominant\n",
        "  elif d == 0:\n",
        "    dom_indices = []   # NO dominant\n",
        "  elif d == num_views:\n",
        "    dom_indices = [i for i in range(num_views)]   # All dominant\n",
        "\n",
        "  for element in knob_non_dom:\n",
        "    temp = []\n",
        "    for i in range(num_views):\n",
        "      if i in dom_indices:\n",
        "        temp.append(knob_dom)\n",
        "      else:\n",
        "        temp.append(element)\n",
        "    knobs.append(temp)\n",
        "  if len(dom_indices) == 0:\n",
        "    return knobs, 'NA'\n",
        "  else:\n",
        "    dom_nodes = [dom_index+1 for dom_index in dom_indices]\n",
        "    return knobs, dom_nodes\n",
        "\n",
        "# Function to get the knob for the non_dominant group\n",
        "def get_knob_non_dom(knobs, knob_dom):\n",
        "  unique_factors = set(knobs)\n",
        "  unique_factor_list = list(unique_factors)\n",
        "  if knob_dom in unique_factor_list:\n",
        "    unique_factor_list.remove(knob_dom)\n",
        "  else:\n",
        "    return knob_dom\n",
        "  if len(unique_factor_list) == 0:\n",
        "    return knob_dom\n",
        "  else:\n",
        "    return unique_factor_list[0]\n",
        "\n",
        "def write_run_summary(test_id, run_dir, name, cnn_name, num_views, n_models_test, part_point, batchSize, cur_batch_size, \\\n",
        "                      num_workers, \\\n",
        "                      load_svcnn, load_mvcnn, modelfile_svcnn, modelfile_mvcnn, subsampling_factor, interpolation, \\\n",
        "                      val_overall_acc, ss_mode, num_dom_nodes, dominant_nodes, compr_method, compr_knob, \\\n",
        "                      loader_mem_orig, loader_mem_compressed, SAD_loader, sf_dom, cf_dom, refresh_interval, ri_dom, \\\n",
        "                      sparsity, sp_dom,\\\n",
        "                      significance_category, significance, current_batch, running_test, total_iterations,\\\n",
        "                      correctly_classified_models):\n",
        "  # Dictionary to log the parameters\n",
        "  if (n_models_test == 0):\n",
        "    log_num_models_t = 29616\n",
        "  else:\n",
        "    log_num_models_t = n_models_test\n",
        "\n",
        "  image_size = [round(224/ssf) for ssf in subsampling_factor]\n",
        "\n",
        "  sf_non_dom = get_knob_non_dom(subsampling_factor, sf_dom)\n",
        "  cf_non_dom = get_knob_non_dom(compr_knob, cf_dom)\n",
        "  ri_non_dom = get_knob_non_dom(refresh_interval, ri_dom)\n",
        "  sp_non_dom = get_knob_non_dom(sparsity, sp_dom)\n",
        "\n",
        "  if (compr_method == 'None') or (loader_mem_compressed == 0) or (loader_mem_orig == 0):\n",
        "    SAD_norm = 0\n",
        "    compr_ratio = 0\n",
        "  else:\n",
        "    SAD_norm = SAD_loader*2/loader_mem_orig\n",
        "    compr_ratio = loader_mem_orig/loader_mem_compressed\n",
        "\n",
        "  # Baseline accuracies dictionary\n",
        "  baseline_acc = {\n",
        "      'alexnet': 0.91572124,\n",
        "      'vgg11': 0.9226094,\n",
        "      'resnet34': 0.95097244\n",
        "  }\n",
        "\n",
        "  log_dict = {\n",
        "              'test_name': name,\n",
        "              'Serial_no.': running_test,\n",
        "              'Total rows': total_iterations,\n",
        "              'test id': test_id,\n",
        "              'pooling point': part_point,\n",
        "\n",
        "              'number_of_models_tested': log_num_models_t,\n",
        "              'correctly_classified_models' : correctly_classified_models,\n",
        "\n",
        "              'Batch Size (MVCNN)': batchSize,\n",
        "              'Curr. batch size' : cur_batch_size,\n",
        "              'Number of workers': num_workers,\n",
        "\n",
        "              'Current_batch': current_batch,\n",
        "              'Sig_category': significance_category,\n",
        "              'significance_order': significance,\n",
        "\n",
        "              'load_svcnn': load_svcnn,\n",
        "              'load_mvcnn': load_mvcnn,\n",
        "              'SVCNN model name': modelfile_svcnn,\n",
        "              'MVCNN model name': modelfile_mvcnn,\n",
        "              'image size': image_size,\n",
        "\n",
        "\n",
        "              # Useful\n",
        "              'cnn': cnn_name,\n",
        "              'number of views': num_views,\n",
        "  }\n",
        "\n",
        "  for i in range(len(subsampling_factor)):\n",
        "    key = 'SSF'+str(i+1)\n",
        "    log_dict[key] = subsampling_factor[i]\n",
        "\n",
        "  # Add Feature Map compression parameters\n",
        "  for i in range(len(compr_knob)):\n",
        "    key = 'CCF'+str(i+1)    # Communication Compression Factor\n",
        "    log_dict[key] = compr_knob[i]\n",
        "\n",
        "  # Add Approx Memory parameters\n",
        "  for i in range(len(refresh_interval)):\n",
        "    key = 'RI'+str(i+1)    # Communication Compression Factor\n",
        "    log_dict[key] = refresh_interval[i]\n",
        "\n",
        "  # Add Approx Cmpute parameters\n",
        "  for i in range(len(sparsity)):\n",
        "    key = 'SP'+str(i+1)    # Communication Compression Factor\n",
        "    log_dict[key] = sparsity[i]\n",
        "\n",
        "  # All approximation knobs\n",
        "  log_dict['interpolation'] = interpolation\n",
        "  log_dict['Compression Method'] = compr_method\n",
        "  log_dict['Original IFM (bytes)'] = loader_mem_orig\n",
        "  log_dict['Compressed IFM (bytes)'] = loader_mem_compressed\n",
        "  log_dict['SAD'] = SAD_loader\n",
        "  log_dict['SAD (Normalized)'] = SAD_norm\n",
        "\n",
        "  log_dict['Choice'] = ss_mode\n",
        "  log_dict['dominant_nodes'] = dominant_nodes\n",
        "  log_dict['d'] = num_dom_nodes\n",
        "  log_dict['SFd'] = sf_dom\n",
        "  log_dict['SFn'] = sf_non_dom\n",
        "  log_dict['CFd'] = cf_dom\n",
        "  log_dict['CFn'] = cf_non_dom\n",
        "  log_dict['RId'] = ri_dom\n",
        "  log_dict['RIn'] = ri_non_dom\n",
        "  log_dict['SPd'] = sp_dom\n",
        "  log_dict['SPn'] = sp_non_dom\n",
        "  log_dict['Compression Ratio'] = compr_ratio\n",
        "  log_dict['Accuracy'] = val_overall_acc\n",
        "  log_dict['Norm. Accuracy'] = float(val_overall_acc) / baseline_acc[cnn_name]\n",
        "\n",
        "  fields = log_dict.keys()\n",
        "  # logfile = run_dir + '/' + run_dir + '_' + cnn_name + '_Sensor_Subsampling_summary.csv'\n",
        "  logfile = run_dir + '/' + 'Raw_log.csv'\n",
        "\n",
        "  with open(logfile, 'a') as csvfile:\n",
        "    csvwriter = csv.DictWriter(csvfile, fieldnames=fields)\n",
        "    if (test_id == 'A1') and (current_batch == 0):\n",
        "      csvwriter.writeheader()\n",
        "    csvwriter.writerow(log_dict)\n",
        "  csvfile.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New helper functions (class-wise)"
      ],
      "metadata": {
        "id": "WBJq9T73gBCD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading significance (class-wise)"
      ],
      "metadata": {
        "id": "EVk-Qmp6WvVI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy.stats import rankdata\n",
        "\n",
        "class drax_significance:\n",
        "  def __init__(self, sig_file_path, sig_methods):\n",
        "    self.sig_file_path = sig_file_path\n",
        "    self.sig_methods = sig_methods\n",
        "    self.df_significance = pd.read_excel(sig_file_path, sheet_name=sig_methods, index_col=None, keep_default_na=False)\n",
        "    self.categories = list(self.df_significance[sig_methods[0]]['class'])\n",
        "\n",
        "  def get_class_significance(self, sig_method, category):\n",
        "    if (sig_method not in self.sig_methods) or (category not in self.categories):\n",
        "      raise ValueError(\"Invalid significance method OR category!\")\n",
        "      return None\n",
        "    else:\n",
        "      values = list(self.df_significance[sig_method].iloc[self.categories.index(category)])[-12:]\n",
        "      # print(values)\n",
        "      ranks = list(rankdata(values).astype(int))\n",
        "      # print(ranks)\n",
        "      return ranks\n",
        "# class_sig = significances.get_class_significance('entropy', 'Average')\n",
        "# print(class_sig)"
      ],
      "metadata": {
        "id": "a_zucIuvm30z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "functions to predict image class (not used) and test one batch"
      ],
      "metadata": {
        "id": "u5VzzF3VrmO0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to update validation accuracy\n",
        "def predict_image_class(cnet, data, view):\n",
        "  # print(data)\n",
        "  ## data[0] contains correct categories for all items in the batch\n",
        "  ## data[0][0] contains correct category for the first item in the batch\n",
        "  correct_category = data[0][0].to(device)\n",
        "  image = torch.unsqueeze(data[1][0][view-1], dim=0).to(device)\n",
        "  # print(image.shape)\n",
        "  out = cnet(image)\n",
        "  # print(out.shape)\n",
        "  pred_category = torch.max(out, 1)[1]\n",
        "\n",
        "  return int(pred_category), int(correct_category)\n",
        "\n",
        "# Function to get accuracy for one single batch\n",
        "def test_one_batch(model, loader, model_name, num_views=12):\n",
        "    # Set the model in \"Evaluation\" mode\n",
        "    model.eval()\n",
        "    batch_mem_orig, batch_mem_compressed, SAD_batch = [0,0,0]\n",
        "    for data_batch in loader:\n",
        "      # Send the data to CUDA based on MVCNN or SVCNN\n",
        "      if model_name == 'mvcnn':\n",
        "          N,V,C,H,W = data_batch[1].size()\n",
        "          in_data = Variable(data_batch[1]).view(-1,C,H,W).to(device)\n",
        "      else:#'svcnn'\n",
        "          in_data = Variable(data_batch[1]).to(device)\n",
        "      target = Variable(data_batch[0]).to(device)\n",
        "\n",
        "      # Get output, calculate loss and result (Correct or Incorrect)\n",
        "      if model_name == 'mvcnn':\n",
        "        out_data, batch_mem_orig, batch_mem_compressed, SAD_batch = model(in_data)   # Updated for feature map compression (Doesn't work with MVCNN.py)\n",
        "      else:\n",
        "        out_data = model(in_data)   # Updated for feature map compression (Doesn't work with MVCNN.py)\n",
        "      pred = torch.max(out_data, 1)[1]\n",
        "      results = pred == target\n",
        "\n",
        "      # The number of correct predictions in the batch\n",
        "      correct_points = torch.sum(results.long())\n",
        "\n",
        "      # Calculate overall acc\n",
        "      acc = correct_points.float() / results.size()[0]\n",
        "      batch_acc = acc.cpu().data.numpy()\n",
        "\n",
        "      # print ('Batch acc. : ', batch_acc)\n",
        "\n",
        "    # Return the logs\n",
        "    return int(correct_points), results.size()[0], batch_acc, batch_mem_orig, batch_mem_compressed, SAD_batch\n",
        "\n",
        "# from torch.utils.data import DataLoader\n",
        "# root_dir = \"/content/modelnet_test/*/test\"\n",
        "# mask_dir = '/content/error_mask'\n",
        "# val_dataset = Multiview_Dataset_Batch(root_dir=root_dir, mask_dir=mask_dir, start_idx=0, end_idx=1)\n",
        "# print(val_dataset[0][2])\n",
        "# print(len(val_dataset))\n",
        "# dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=2)\n",
        "# # # Iterate over the dataloader to access the data in batches\n",
        "# # for i, batch in enumerate(dataloader):\n",
        "# #     print(batch)\n",
        "\n",
        "# ## Load the pretrained Single View CNN (SVCNN)\n",
        "# cnn_name = name = 'resnet34'\n",
        "# part_point = 5\n",
        "# accurate_model_dir = '/content/drive/MyDrive/Arghadip/MVCNN/jongchyisu/mvcnn_pytorch_analysis/0_Journal/Logs/Training_aligned_modelnet/Version_2023_May_24/resnet34/resnet34_stage_1/resnet34'\n",
        "# modelfile_svcnn = 'model-00010.pth'\n",
        "# view_for_prediction = 1\n",
        "\n",
        "# cnet = SVCNN(name, nclasses=40, pretraining=False, cnn_name=cnn_name, part_point=part_point)\n",
        "# cnet.load(accurate_model_dir, modelfile_svcnn) # Load model parameters <FIXME>\n",
        "# cnet = cnet.to(device)\n",
        "\n",
        "# # for view in range(1,13):\n",
        "# for i, batch in enumerate(dataloader):\n",
        "#     pred_category, correct_category = predict_image_class(cnet, batch, view_for_prediction)\n",
        "#     print(pred_category, correct_category)"
      ],
      "metadata": {
        "id": "yNJkIzzDdUdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# batchSize_SV = 1\n",
        "# num_workers = 2\n",
        "# optimizer = optim.Adam(cnet.parameters(), lr=5e-5, weight_decay=0.001)\n",
        "# val_dataset = SingleImgDataset(root_dir, scale_aug=False, rot_aug=False, num_models=0, test_mode=True)\n",
        "# val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batchSize_SV, shuffle=False, num_workers=num_workers)\n",
        "# log_dir = '/content/drive/MyDrive/Arghadip/MVCNN/jongchyisu/mvcnn_pytorch_analysis/0_Journal/Logs/Debug'\n",
        "# trainer = ModelNetTrainer(cnet, val_loader, val_loader, optimizer, nn.CrossEntropyLoss(), 'svcnn', log_dir, num_views=1)\n",
        "# trainer.update_validation_accuracy(1)"
      ],
      "metadata": {
        "id": "eYOqNMiQpP_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load models"
      ],
      "metadata": {
        "id": "6J2hOsajrtDh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_names (ri_dom, sp_dom, refresh_interval, sparsity):\n",
        "  ## Get the unique list of sparsities and refresh intervals\n",
        "  unique_intervals_list = list(set(refresh_interval))\n",
        "  unique_sparsities_list = list(set(sparsity))\n",
        "\n",
        "  ## Initialization\n",
        "  file_name_dom = file_name_nondom = None\n",
        "\n",
        "  ## Return file names for dom and non-dom groups\n",
        "  if (len(unique_intervals_list) == 1) and (len(unique_sparsities_list) == 1):\n",
        "    ## If accurate\n",
        "    ## NOT covered here, loaded every-time\n",
        "\n",
        "    # else if approximate but all equal (load once)\n",
        "    if ((unique_intervals_list[0] == 0) and (not (unique_sparsities_list[0] == 0))):\n",
        "      file_name_dom = 'approx_' + cnn_name + '_PP_' + str(part_point) + '_dram' + str(1) + '_sparsity_' + str(unique_sparsities_list[0]) + '.pth.tar'    # RI = 0 and 1 has no errors\n",
        "    else:\n",
        "      file_name_dom = 'approx_' + cnn_name + '_PP_' + str(part_point) + '_dram' + str(unique_intervals_list[0]) + '_sparsity_' + str(unique_sparsities_list[0]) + '.pth.tar'\n",
        "\n",
        "    file_name_nondom = file_name_dom\n",
        "\n",
        "  elif (len(unique_intervals_list) <= 2) and (len(unique_sparsities_list) <= 2):   # When operating with group-wise same knob settings\n",
        "    ri_nondom = _get_non_dom_knob (unique_intervals_list, ri_dom)\n",
        "    sp_nondom = _get_non_dom_knob (unique_sparsities_list, sp_dom)\n",
        "    file_name_dom = 'approx_' + cnn_name + '_PP_' + str(part_point) + '_dram' + str(ri_dom) + '_sparsity_' + str(sp_dom) + '.pth.tar'\n",
        "    file_name_nondom = 'approx_' + cnn_name + '_PP_' + str(part_point) + '_dram' + str(ri_nondom) + '_sparsity_' + str(sp_nondom) + '.pth.tar'\n",
        "\n",
        "  else:\n",
        "    pass\n",
        "\n",
        "  return file_name_dom, file_name_nondom\n",
        "\n",
        "def load_relevant_models (cnn_name, part_point, sps, ris, approx_model_dir):\n",
        "  approx_model_dict = {}\n",
        "  for sp in sps:\n",
        "    for ri in ris:\n",
        "      filename = 'approx_' + cnn_name + '_PP_' + str(part_point) + '_dram' + str(ri) + '_sparsity_' + str(sp) + '.pth.tar'\n",
        "      ## If no such file exists\n",
        "      if filename not in os.listdir(approx_model_dir):\n",
        "        raise FileNotFoundError(\"File does not exist: {}\".format(filename))\n",
        "      ## Else\n",
        "      else:\n",
        "        temp = distiller.models.create_model(pretrained=True, dataset='modelnet', arch= cnn_name + '_mvcnn', parallel=False, device_ids=-1)\n",
        "        load_checkpoint(temp, os.path.join(approx_model_dir,filename))\n",
        "        print('Loaded model from ', filename)\n",
        "        temp.eval()\n",
        "        ## Add model parameters to the dictionary\n",
        "        approx_model_dict[filename] = temp\n",
        "        del temp\n",
        "  # print(approx_model_dict.keys())\n",
        "  return approx_model_dict"
      ],
      "metadata": {
        "id": "Qn4kBnYGBQR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cnn_name = 'resnet34'\n",
        "# part_point = 5\n",
        "# sps = [1.0, 2.5]\n",
        "# ris = [1,10]\n",
        "# approx_model_dir = '/content/Approx_models'\n",
        "# approx_models = load_relevant_models (cnn_name, part_point, sps, ris, approx_model_dir)"
      ],
      "metadata": {
        "id": "Lgb_Wd0LKYEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "New logging helpers"
      ],
      "metadata": {
        "id": "7dK27_ysrzUV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_timing_log(run_dir, test_id, cnn_name, num_views, timer0_0, timer0_1, timer1_0, timer1_1, timer1_2, \\\n",
        "                        timer2_0, timer2_1, timer2_2, timer2_3, timer2_4):\n",
        "  ## Form a dictionary to log the timings\n",
        "  timing_dict = {\n",
        "      'TID': test_id,\n",
        "      'CNN': cnn_name,\n",
        "      'Num_views': num_views,\n",
        "      't_dataset_init': (timer0_1 - timer0_0).total_seconds(),\n",
        "      't_sig_det': (timer1_1 - timer1_0).total_seconds(),\n",
        "      't_knob_gen': (timer1_2 - timer1_1).total_seconds(),\n",
        "      't_CNN': (timer2_1 - timer2_0).total_seconds(),\n",
        "      't_dataset_batch': (timer2_2 - timer2_1).total_seconds(),\n",
        "      't_testing': (timer2_3 - timer2_2).total_seconds(),\n",
        "      't_logging': (timer2_4 - timer2_3).total_seconds(),\n",
        "  }\n",
        "  fields = timing_dict.keys()\n",
        "  logfile = run_dir + '/' + 'Timing_log.csv'\n",
        "\n",
        "  with open(logfile, 'a') as csvfile:\n",
        "    csvwriter = csv.DictWriter(csvfile, fieldnames=fields)\n",
        "    if (test_id == 'A1') and (current_batch == 0):\n",
        "      csvwriter.writeheader()\n",
        "    csvwriter.writerow(timing_dict)\n",
        "  csvfile.close()\n",
        "\n",
        "\n",
        "def generate_summary_log(run_dir):\n",
        "  ### Forming the summary log file\n",
        "  csv_path = os.path.join(run_dir,'Raw_log.csv')\n",
        "  data = pd.read_csv(csv_path)\n",
        "  # Baseline accuracies dictionary\n",
        "  baseline_acc = {\n",
        "      'alexnet': 0.91572124,\n",
        "      'vgg11': 0.9226094,\n",
        "      'resnet34': 0.95097244\n",
        "  }\n",
        "  ## Get the all possible test ids\n",
        "  TIDs = data['test id'].unique()\n",
        "  print(TIDs)\n",
        "  test_counter = 0\n",
        "  for TID in TIDs:\n",
        "    test_counter += 1\n",
        "    # Get only the row where the TID matches\n",
        "    data_tid = data[data['test id'] == TID]\n",
        "    # Form a dictionary with the first row\n",
        "    log_dict = data_tid.iloc[0]['cnn':'Norm. Accuracy'].to_dict()\n",
        "    # Update 'Original IFM (bytes)', 'Compressed IFM (bytes)', 'SAD', 'SAD (Normalized)', 'dominant_nodes', 'Compression Ratio', 'Accuracy', 'Norm. Accuracy'\n",
        "    log_dict['Original IFM (bytes)'] = data_tid['Original IFM (bytes)'].sum()\n",
        "    log_dict['Compressed IFM (bytes)'] = data_tid['Compressed IFM (bytes)'].sum()\n",
        "    log_dict['SAD'] = data_tid['SAD'].sum()\n",
        "\n",
        "    if (log_dict['Compression Method'] == 'None') or (log_dict['Original IFM (bytes)'] == 0) or (log_dict['Compressed IFM (bytes)'] == 0):\n",
        "      log_dict['SAD (Normalized)'] = 0\n",
        "      log_dict['Compression Ratio'] = 0\n",
        "    else:\n",
        "      log_dict['SAD (Normalized)'] = log_dict['SAD']*2/log_dict['Original IFM (bytes)']\n",
        "      log_dict['Compression Ratio'] = log_dict['Original IFM (bytes)']/log_dict['Compressed IFM (bytes)']\n",
        "\n",
        "    log_dict['dominant_nodes'] = 'Batchwise'\n",
        "\n",
        "    total_models = data_tid['number_of_models_tested'].sum()\n",
        "    correctly_classified = data_tid['correctly_classified_models'].sum()\n",
        "    log_dict['Accuracy'] = correctly_classified/total_models\n",
        "    log_dict['Norm. Accuracy'] = log_dict['Accuracy']/baseline_acc[log_dict['cnn']]\n",
        "\n",
        "    fields = log_dict.keys()\n",
        "    # logfile = run_dir + '/' + run_dir + '_' + cnn_name + '_Sensor_Subsampling_summary.csv'\n",
        "    logfile = run_dir + '/' + 'Run_summary.csv'\n",
        "\n",
        "    with open(logfile, 'a') as csvfile:\n",
        "      csvwriter = csv.DictWriter(csvfile, fieldnames=fields)\n",
        "      if (test_counter == 1):\n",
        "        csvwriter.writeheader()\n",
        "      csvwriter.writerow(log_dict)\n",
        "    csvfile.close()"
      ],
      "metadata": {
        "id": "3PLZChx9oRUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Category-wise significance"
      ],
      "metadata": {
        "id": "tnN8U1JlmtMN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "User inputs"
      ],
      "metadata": {
        "id": "MircmN01spjY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "########################## Primary Inputs ######################################\n",
        "cnn_name = \"resnet34\" #\"alexnet\"   # \"vgg11\"\n",
        "expt_name = 'cat_en_corr'\n",
        "# Batching\n",
        "batchSize = 50\n",
        "\n",
        "########################## CHANGE THIS ######################################\n",
        "## Method to determine significance\n",
        "significance_method = 'entropy'\n",
        "# View for significance\n",
        "view_to_determine_sig = None\n",
        "\n",
        "## Grouping strategy\n",
        "## \"best\" = SAGA\n",
        "## \"worst\" = Anti-SAGA\n",
        "## \"random\" = In between SAGA and Anti-SAGA\n",
        "mode_list = ['best'] #['best', 'random', 'worst']\n",
        "\n",
        "## Log timing or not\n",
        "log_timing = False\n",
        "\n",
        "# d\n",
        "num_dom_nodes = [1, 2, 4, 6]\n",
        "# SPd\n",
        "sps_dom = [2.5] # 5.0\n",
        "# RId\n",
        "ris_dom = [1 10, 20]\n",
        "\n",
        "# SFd\n",
        "sfs_dom = [1, 1.15, 1.2]\n",
        "# CFd\n",
        "cfs_dom = [5, 10, 20]\n",
        "\n",
        "compr_method_list = ['zfp'] #'fpzip'"
      ],
      "metadata": {
        "id": "COU5KebpsFfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code body"
      ],
      "metadata": {
        "id": "HWEeargAsq-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###################### High level parameters ##################################\n",
        "###############################################################################\n",
        "tz = timezone('US/Eastern')\n",
        "now = datetime.now(tz)\n",
        "\n",
        "part_points_dict = {\n",
        "                  'alexnet' : 5,\n",
        "                  'vgg11'   : 10,\n",
        "                  'resnet34': 5\n",
        "}\n",
        "\n",
        "accurate_models_dict = {\n",
        "                  'alexnet_PP_5' : 'Alexnet_MVCNN_PP_5_pkl.pt',\n",
        "                  'vgg11_PP_10'  : 'Vgg11_MVCNN_PP_10_pkl.pt',\n",
        "                  'resnet34_PP_5': 'Resnet34_MVCNN_PP_5_pkl.pt'\n",
        "}\n",
        "\n",
        "#############################################################################\n",
        "# SFn\n",
        "sf_non_dom = sfs_dom #[1.1,1.15]\n",
        "\n",
        "# CFn\n",
        "cf_non_dom = cfs_dom #[5,10]\n",
        "\n",
        "# RIn\n",
        "ri_non_dom = [1, 10, 20]\n",
        "\n",
        "# SPn\n",
        "sp_non_dom = sps_dom    # Must be the same\n",
        "\n",
        "################################# Secondary Inputs #############################\n",
        "## Significance\n",
        "sig_file_path = '/category_wise_significance.xlsx'\n",
        "sig_methods = ['entropy']\n",
        "significances = drax_significance(sig_file_path, sig_methods)\n",
        "\n",
        "num_workers = 2\n",
        "part_point = part_points_dict[cnn_name]\n",
        "num_views = 12\n",
        "name = cnn_name\n",
        "\n",
        "run_name = cnn_name + '_' + expt_name + '_PP_' + str(part_point) + '_' + str(now.strftime('%Y-%m-%d_%H:%M:%S'))\n",
        "\n",
        "# run_dir = '/content/drive/MyDrive/Arghadip/MVCNN/jongchyisu/mvcnn_pytorch_analysis/0_Journal/Logs/Classwise_significance/' + run_name\n",
        "run_dir = '/data/' + run_name\n",
        "\n",
        "# Entire test list\n",
        "# significance = significances_dict[cnn_name + '_PP_' + str(part_point)]\n",
        "interpolations = ['nearest'] #, 'bilinear', 'bicubic']\n",
        "covered_drax_space = []\n",
        "mask_dir = '/error_mask'\n",
        "approx_model_dir = \"/Approx_models\"\n",
        "approx_models = load_relevant_models (cnn_name, part_point, sps_dom, ri_non_dom, approx_model_dir)\n",
        "\n",
        "# Number of 3D models to test, 0 means the whole dataset\n",
        "n_models_test = 0\n",
        "# n_models_test = X * num_views\n",
        "\n",
        "accurate_model_dir= '/acc_models'\n",
        "\n",
        "svcnn_load_path = 'dummy'\n",
        "modelfile_svcnn = 'model-00010.pth'\n",
        "modelfile_mvcnn = accurate_models_dict[cnn_name + '_PP_' + str(part_point)]\n",
        "\n",
        "###################### Infrequent parameters ##################################\n",
        "###############################################################################\n",
        "# Parameters which are not frequently changed\n",
        "lr = 5e-5\n",
        "weight_decay = 0.001\n",
        "no_pretraining = True\n",
        "val_path = \"/modelnet_test/*/test\"\n",
        "\n",
        "###################### Helper functions #######################################\n",
        "###############################################################################\n",
        "# Defined function to create log directory\n",
        "def create_folder(log_dir):\n",
        "    # make summary folder\n",
        "    if not os.path.exists(log_dir):\n",
        "        os.mkdir(log_dir)\n",
        "    else:\n",
        "        print('WARNING: summary folder already exists!! It will be overwritten!!')\n",
        "        shutil.rmtree(log_dir)\n",
        "        os.mkdir(log_dir)\n",
        "\n",
        "create_folder(run_dir)\n",
        "\n",
        "test_id_1 = 0\n",
        "running_test = 0\n",
        "## Load the pretrained Single View CNN (SVCNN)\n",
        "cnet = SVCNN(name, nclasses=40, pretraining=False, cnn_name=cnn_name, part_point=part_point)\n",
        "if view_to_determine_sig is not None:\n",
        "  cnet.load(svcnn_load_path, modelfile_svcnn) # Load model parameters\n",
        "  cnet = cnet.to(device)\n",
        "\n",
        "for ss_mode in mode_list:\n",
        "  for num_dom_node in num_dom_nodes:\n",
        "    for compr_method in compr_method_list:\n",
        "      for interpolation in interpolations:\n",
        "        ###################### Derived parameters #####################################\n",
        "        ###############################################################################\n",
        "        # Derived parameters\n",
        "        # n_models_test =  n_models_test * num_views\n",
        "        if (interpolation == 'nearest'):\n",
        "          interpolation_mode = InterpolationMode.NEAREST\n",
        "          # interpolation_mode = Image.NEAREST\n",
        "        elif (interpolation == 'bilinear'):\n",
        "          interpolation_mode = InterpolationMode.BILINEAR\n",
        "          # interpolation_mode = Image.BILINEAR\n",
        "        elif (interpolation == 'bicubic'):\n",
        "          interpolation_mode = InterpolationMode.BILINEAR\n",
        "          # interpolation_mode = Image.BICUBIC\n",
        "        for sp_dom in sps_dom:\n",
        "          for ri_dom in ris_dom:\n",
        "            for cf_dom in cfs_dom:\n",
        "              for sf_dom in sfs_dom:\n",
        "                test_id_1 += 1\n",
        "                test_id_1_chr = chr(test_id_1 + 64)   # 64 is added to start with 'A'\n",
        "\n",
        "                ## TO-DO1: TRIGGER THE SIGNIFICANCE DETERMINATION STEP IN EVERY FEW BATCHES\n",
        "                ## TO-DO2: PUT A BYPASS LOOP FOR THE BATCH-WISE SIGNIFICANCE DETERMINATION (LOW PRIORITY)\n",
        "\n",
        "                ## Timer\n",
        "                if log_timing:\n",
        "                  timer0_0 = datetime.now(tz)\n",
        "\n",
        "                ## Load the whole ModelNet40 dataset (NO shuffling)\n",
        "                val_dataset = Multiview_Dataset_Batch(val_path, scale_aug=False, rot_aug=False, test_mode=True,\n",
        "                                            num_models=n_models_test, num_views=num_views, shuffle=False,\n",
        "                                            mask_dir=mask_dir, interpolation=interpolation_mode)\n",
        "                print(f\"Whole val dataset content is {len(val_dataset)}\")\n",
        "                # Create a sampler\n",
        "                batch_sampler = DRAX_Classwise_Batch_Sampler(class_counts=val_dataset.num_models_per_category, batch_size=batchSize)\n",
        "                ## Create a dataloader (NO shuffling)\n",
        "                val_loader = torch.utils.data.DataLoader(val_dataset, batch_sampler=batch_sampler, shuffle=False, num_workers=num_workers)\n",
        "                total_batches = len(val_loader)\n",
        "\n",
        "                # dd/mm/YY H:M:S\n",
        "                timer0_1 = datetime.now(tz)\n",
        "                dt_string = timer0_1.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
        "                print(\"date and time =\", dt_string)\n",
        "\n",
        "                ## Start looping through each batch\n",
        "                start_index = 0\n",
        "                for current_batch, data in enumerate(val_loader):\n",
        "                  ## For each batch, in stage-1, we predict the class of the first model in the batch\n",
        "                  # print(\"################ STARTING STAGE 1 ###################\")\n",
        "\n",
        "                  ## Get the end index of the dataset\n",
        "                  cur_batch_size = len(data[0])\n",
        "                  end_index = start_index + cur_batch_size\n",
        "\n",
        "                  if log_timing:\n",
        "                    timer1_0 = datetime.now(tz)\n",
        "\n",
        "                  ## Pass the 2D image and cnet to the test function, it returns the PREDICTED class of the image\n",
        "                  ## Select a particular view (the one which is the overall most significanct)\n",
        "                  if view_to_determine_sig is not None:\n",
        "                    pred_category, correct_category = predict_image_class(cnet, data, view_to_determine_sig)\n",
        "                    significance_category = val_dataset.classnames[pred_category]\n",
        "                  else:\n",
        "                    correct_category = int(data[0][0].to(device))\n",
        "                    significance_category = val_dataset.classnames[correct_category]\n",
        "\n",
        "                  ## Get the class-wise significance using the predicted class\n",
        "                  significance = significances.get_class_significance(significance_method, significance_category)\n",
        "\n",
        "                  if log_timing:\n",
        "                    timer1_1 = datetime.now(tz)\n",
        "\n",
        "                  ## Generate the combination of approximation knobs using the given significance of views\n",
        "                  sparsities, dominant_nodes = generate_groupwise_knobs (num_views, num_dom_node, sp_dom, sp_non_dom, significance, ss_mode)\n",
        "                  refresh_intervals, dominant_nodes = generate_groupwise_knobs (num_views, num_dom_node, ri_dom, ri_non_dom, significance, ss_mode)\n",
        "                  compr_knob_list, dominant_nodes = generate_groupwise_knobs (num_views, num_dom_node, cf_dom, cf_non_dom, significance, ss_mode)\n",
        "                  subsampling_factors, dominant_nodes = generate_groupwise_knobs(num_views, num_dom_node, sf_dom, sf_non_dom, significance, ss_mode)\n",
        "\n",
        "                  if log_timing:\n",
        "                    timer1_2 = datetime.now(tz)\n",
        "\n",
        "                  experiments_per_batch = int(len(refresh_intervals)*len(compr_knob_list)*len(subsampling_factors)*len(sparsities)/len(sps_dom))\n",
        "\n",
        "                  ## Start exploring all combination of approx knob settings for this particular batch\n",
        "                  test_id_2 = 0\n",
        "                  for sparsity in sparsities:\n",
        "                    for refresh_interval in refresh_intervals:\n",
        "                      for compr_knob in compr_knob_list:\n",
        "                        for subsampling_factor in subsampling_factors:\n",
        "                          if (len(set(sparsity)) != 1) or ([ss_mode, num_dom_node, compr_method, interpolation, sp_dom, ri_dom, cf_dom, sf_dom, sparsity, refresh_interval, compr_knob, subsampling_factor] in covered_drax_space):\n",
        "                            continue\n",
        "                          # print(subsampling_factor)\n",
        "                          # Test id\n",
        "                          total_iterations = int(len(compr_method_list)*len(mode_list)*len(ris_dom)*len(cfs_dom)*len(sfs_dom)\\\n",
        "                                            *len(sps_dom)*len(num_dom_nodes)*len(refresh_intervals)*len(compr_knob_list)\\\n",
        "                                            *len(interpolations)*len(subsampling_factors)*len(sparsities)*total_batches/len(sps_dom))\n",
        "\n",
        "                          test_id_2 = test_id_2 + 1\n",
        "                          test_id = test_id_1_chr + str(test_id_2)\n",
        "\n",
        "                          running_test += 1\n",
        "\n",
        "                          # STAGE 2\n",
        "                          # print(\"################ STARTING STAGE 2 ###################\")\n",
        "\n",
        "                          if log_timing:\n",
        "                            timer2_0 = datetime.now(tz)\n",
        "\n",
        "                          # Load CNN model using MVCNN model\n",
        "                          modelname_dom, modelname_nondom = get_model_names (ri_dom, sp_dom, refresh_interval, sparsity)\n",
        "                          cnet_2 = MVCNN_approx_opt(name, cnet, nclasses=40, accurate_models_dict=accurate_models_dict,\\\n",
        "                                                    cnn_name=cnn_name, num_views=num_views, part_point=part_point, \\\n",
        "                                                    compr_method=compr_method, compr_knob=compr_knob, \\\n",
        "                                                    refresh_interval=refresh_interval, \\\n",
        "                                                    approx_model_dir=approx_model_dir, \\\n",
        "                                                    accurate_model_dir=accurate_model_dir, sparsity=sparsity, \\\n",
        "                                                    ri_dom=ri_dom, sp_dom=sp_dom,\\\n",
        "                                                    model_dom=approx_models[modelname_dom], model_nondom=approx_models[modelname_nondom]).to(device)\n",
        "\n",
        "                          if log_timing:\n",
        "                            timer2_1 = datetime.now(tz)\n",
        "\n",
        "                          modelfile_mvcnn = cnet_2.file_name\n",
        "\n",
        "                          ## Reload the batch images (all views) with proper sensor and memory approx\n",
        "                          val_dataset_batch = Multiview_Dataset_Batch(val_path, scale_aug=False, rot_aug=False, test_mode=True,\n",
        "                                              num_models=n_models_test, num_views=num_views, subsampling_factor=subsampling_factor,\n",
        "                                              refresh_interval=refresh_interval, mask_dir=mask_dir, interpolation=interpolation_mode,\n",
        "                                              shuffle=False, start_idx=start_index, end_idx=end_index)\n",
        "                          # print(f\"val dataset content is {len(val_dataset_batch)}\")\n",
        "                          ## Validation loader (no sampler needed, taken care of by using start and end index)\n",
        "                          val_loader_batch = torch.utils.data.DataLoader(val_dataset_batch, batch_size=cur_batch_size, shuffle=False, num_workers=num_workers)\n",
        "                          # print(val_loader_batch.batch_size, val_dataset_batch[0][0], val_dataset_batch[cur_batch_size-1][0])\n",
        "                          # print('num_val_files: '+str(len(val_dataset_batch.filepaths)))\n",
        "\n",
        "                          if log_timing:\n",
        "                            timer2_2 = datetime.now(tz)\n",
        "\n",
        "                          ## Test the models, returns category-wise total and correctly classified models\n",
        "                          correctly_classified_models , num_test_models, val_overall_acc, loader_mem_orig, loader_mem_compressed, SAD_loader = \\\n",
        "                          test_one_batch(cnet_2, val_loader_batch, 'mvcnn', num_views=num_views)\n",
        "\n",
        "                          ## Update class-wise running total and correctly classified models\n",
        "\n",
        "                          # print('Itr ', running_test, '/', total_iterations, ' | TID: ', test_id, \\\n",
        "                          #       ' | Batch: ', current_batch, ' | Sig_Cat: ', significance_category, ' | Accuracy: ', val_overall_acc)\n",
        "\n",
        "                          if log_timing:\n",
        "                            timer2_3 = datetime.now(tz)\n",
        "\n",
        "                          # Dump to run summary\n",
        "                          write_run_summary(test_id, run_dir, expt_name, cnn_name, num_views, num_test_models,\n",
        "                                            part_point, batchSize, val_loader_batch.batch_size, num_workers, True,\n",
        "                                            True, modelfile_svcnn, modelfile_mvcnn,\n",
        "                                            subsampling_factor, interpolation,\n",
        "                                            val_overall_acc, ss_mode, num_dom_node, dominant_nodes, compr_method, compr_knob,\n",
        "                                            loader_mem_orig, loader_mem_compressed, SAD_loader, sf_dom, cf_dom,\n",
        "                                            refresh_interval, ri_dom, sparsity, sp_dom,\n",
        "                                            significance_category, significance, current_batch, running_test, total_iterations,\n",
        "                                            correctly_classified_models)\n",
        "\n",
        "                          if log_timing:\n",
        "                            timer2_4 = datetime.now(tz)\n",
        "                            generate_timing_log(run_dir, test_id, cnn_name, num_views, timer0_0, timer0_1, timer1_0, timer1_1, timer1_2, \\\n",
        "                            timer2_0, timer2_1, timer2_2, timer2_3, timer2_4)\n",
        "\n",
        "                  print('Itr ', running_test, '/', total_iterations, ' | Batch ', current_batch+1,  '/', total_batches, ' | Sig_Cat: ', significance_category)\n",
        "                  start_index = end_index\n",
        "\n",
        "                final_now = datetime.now(tz)\n",
        "                dt_string = final_now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
        "                print(f'Finished test {test_id_1} at {dt_string}')\n",
        "                duration = final_now - timer0_1\n",
        "\n",
        "                ## Duration per test\n",
        "                duration_in_s = duration.total_seconds()/experiments_per_batch\n",
        "                days    = divmod(duration_in_s, 86400)        # Get days (without [0]!)\n",
        "                hours   = divmod(days[1], 3600)               # Use remainder of days to calc hours\n",
        "                minutes = divmod(hours[1], 60)                # Use remainder of hours to calc minutes\n",
        "                seconds = divmod(minutes[1], 1)               # Use remainder of minutes to calc seconds\n",
        "                print(\"Time taken (per test): %d days, %d hours, %d minutes and %d seconds\" % (days[0], hours[0], minutes[0], seconds[0]))\n",
        "\n",
        "                # del val_dataset\n",
        "                # del val_loader\n",
        "\n",
        "## Update the overall accuracy and compression details for a particular combination of knob settings\n",
        "generate_summary_log(run_dir)\n",
        "print('\\n\\n\\n\\nExperiments are completed!')"
      ],
      "metadata": {
        "id": "DowpxsXemye4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "KMwewYOMH14S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}